from file import File
import os

#data containers
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#classifier models/tools
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC  
from sklearn import tree
from sklearn.model_selection import KFold # import KFold
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import scale
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectKBest #for feature selection
from sklearn.feature_selection import chi2

import tensorflow as tf
from tensorflow import keras
#import keras
#from keras.models import load_model
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D
from keras.layers import LSTM
from keras.layers import SimpleRNN
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from numpy import array
from numpy import asarray
from numpy import zeros

#for getting data from csv
import csvWrite
import fileRetriever
import bagOfWords

import math
from embeddingSettings import EmbeddingSettings

#for word2vec
import gensim

#saving/loading models
import pickle

#garbage collection
import gc


#free up memory
gc.collect()

def cross_validation(modelType, X, y, embeddingSettings):
    '''Cross validation for various model types. modelType is a string for the model being used'''
    # prepare the k-fold cross-validation configuration
    n_folds = 10
    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle

    # cross validation estimation of performance
    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()

    counter = 1
    for train_ix, test_ix in kfold.split(X):
        print("Cross validation iteration", counter)
        counter = counter + 1
        
        # select samples
        trainX, trainy = X[train_ix], y[train_ix]
        testX, testy = X[test_ix], y[test_ix]
        # evaluate model
        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)

        print("trainX shape:", trainX.shape)
        print("testX shape:", testX.shape)
        print("trainy shape:", trainy.shape)
        print("testy shape:", testy.shape)
        
        if modelType == "mlp":
            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(trainX, trainy, testX, testy, embeddingSettings)
        
        scores.append(test_acc)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1_score)
        members.append(model)
        
        #free up memory
        gc.collect()

    # summarize expected performance
    print(modelType, "Performance")
    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))
    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))
    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))
    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))

def evaluate_model_mlp(X_train, y_train, X_test, y_test, embeddingSettings):
    # define model
    #create MLP model object and train it with the data
    model = Sequential()
    #may need to add input_length, since there is a Flatten() and Dense layer
    e = Embedding(embeddingSettings.vocab_size, embeddingSettings.target_dimension, weights=embeddingSettings.weights, input_length=embeddingSettings.input_length)
    model.add(e)
    model.add(Flatten())
    model.add(Dense(500, activation='relu'))
    model.add(Dense(16, activation='relu'))
    #model.add(Dense(1, activation='sigmoid'))
    model.add(Dense(2, activation='sigmoid'))

    #compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    #fit model
    #switch epochs to 10
    history = model.fit(X_train, y_train, epochs=10)

    # save model
    '''filename = 'mlp.sav'
    pickle.dump(model, open(filename, 'wb'))'''
    '''if featureType == "bagOfWords":
        filename = 'mlp.h5'
        model.save(filename)
    elif featureType == "word2vec":
        filename = 'mlp_word2vec.h5'
        model.save(filename)
    else:
        print("Classifier not saved")'''
    
    #evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    predicted = np.argmax(predict_test, axis=1)
    
    print(y_test)
    print(predict_test)
    print(y_test.shape)
    print(predict_test.shape)
    print(X_train.shape)
    print(X_test.shape)
    
    report = metrics.classification_report(np.argmax(y_test, axis=1), predicted) #report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)

    #Accuracy
    try:
        test_loss, test_acc = model.evaluate(X_test, y_test)
        print('Accuracy: %f' % (test_acc*100))
    except Exception as inst:
        print(inst)
    #test_acc = metrics.accuracy_score(y_test, predict_test)
    #precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    #recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    #f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score


#returns a list of strings. Each string is a file's text
def getCorpus():
    files = fileRetriever.getFileSet(2)
    corpus = []
    for i, file in enumerate(files):
        wordList = []
        with open(file.dir, "r", encoding="utf8") as f:
            text = f.read()
            text = bagOfWords.filterComments(text)
            #wordList = gensim.utils.simple_preprocess(text) #maybe add min_len and max_len params
            #corpus.append(wordList)
            corpus.append(text) # add the string to the corpus

    return corpus

#PROGRAM START
print("Welcome to the SQL injection machine learning classifier!")
loop = True

while loop:
    #printMenu()
    choice = int(input("Enter your choice: "))

    if choice == 1:
        print("Will retrain classifier")

        print("Rewrite fileInfo.csv? Contains feature information for file set ")
        csvWrite.rewriteCSV()

        #get data frame of features
        csvDirectory = csvWrite.getCSVDirectory()
        data_frame = pd.read_csv(csvDirectory)    
            
        #####Loading c2v Model#####
        #get corpus
        print("Retrieving corpus...")
        corpus = getCorpus()

        #####Establish X and y data sets#####
        X = np.array(corpus)
        #X = corpus
        
        
        ####preprocess X ####
        t = Tokenizer()
        #creates vocabulary based on words in X (The tokens in all files)
        t.fit_on_texts(X)
        vocab_size = len(t.word_index) + 1
        #integer encode the documents (turn the words into sequences of ints)
        print("Converting documents words into sequence of integers")
        encoded_docs = t.texts_to_sequences(X)
        print(encoded_docs)
        #pad documents to a numpy array, with everything the same length
        print("Make the inputs the same length")
        max_length = 50
        padded_docs = pad_sequences(encoded_docs, padding='post', maxlen=max_length)
        print(padded_docs)
        #load c2v embedding into memory (dimension is num features)
        #IMPORTANT: need to adjust this based on num features in targets.txt
        target_dimension = 384
                
        print("loading c2v embedding...")
        embeddings_index = dict()
        f = open('targets.txt', encoding='utf8')
        for line in f:
            values = line.split()
            word = values[0]
            coefs = asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
        f.close()
        print('Loaded %s word vectors.' % len(embeddings_index))
    
        #create a weight matrix for each word in the training dataset
        embedding_matrix = zeros((vocab_size, target_dimension)) #x-axis: number of words, y-axis: number of dimensions
        for word, i, in t.word_index.items():
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[i] = embedding_vector
                
        embeddingSettings = EmbeddingSettings(vocab_size, target_dimension, embedding_matrix, max_length)
        
        ###### Handle the Y data set ######
        #get expected values
        predicted_class_names = ['Vulnerable']

        #X = data_frame[feature_col_names].values # predictor feature columns (17 x m)
        y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

        #get a nonvulnerable column for y labels
        y_flipped = np.array(y, copy=True)
        for i in range(len(y_flipped)):
            if y_flipped[i][0] == 0:
                y_flipped[i][0] = 1
            elif y_flipped[i][0] == 1:
                y_flipped[i][0] = 0
                
        y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner
        print("Starting classifier training")

        cross_validation("mlp", padded_docs, y, embeddingSettings)

    elif choice == 3:
        print("Quitting program")
        loop = False
    else:
        print("Input not understood. Please try again")

