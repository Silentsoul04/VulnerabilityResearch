{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7698541283607483)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model. I think this is already trained\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1 #ensure fast performance. Need to download C compiler, like MinGW\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn import metrics\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest #for feature selection\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "import csvWrite\n",
    "import fileRetriever\n",
    "import bagOfWords\n",
    "\n",
    "#do plotting inline\n",
    "%matplotlib inline\n",
    "\n",
    "#Get data frame\n",
    "#get directory\n",
    "csvDirectory = csvWrite.getCSVDirectory()\n",
    "\n",
    "data_frame = pd.read_csv(csvDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\VulnerabilityResearch\\FileSet\n",
      "C:\\VulnerabilityResearch\\julietFiles\n",
      "9709\n",
      "9709\n",
      "9709\n"
     ]
    }
   ],
   "source": [
    "#get corpus\n",
    "'''\n",
    "files = fileRetriever.getFileSet(2)\n",
    "corpus = []\n",
    "for i, file in enumerate(files):\n",
    "    wordList = []\n",
    "    with open(file.dir, \"r\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        wordList = gensim.utils.simple_preprocess(text) #maybe add min_len and max_len params\n",
    "        corpus.append(wordList)\n",
    "'''\n",
    "def getCorpus(fileSetNum, filterComments=True):\n",
    "    files = fileRetriever.getFileSet(fileSetNum)\n",
    "    corpus = []\n",
    "    for i, file in enumerate(files):\n",
    "        wordList = []\n",
    "        with open(file.dir, \"r\", encoding=\"utf8\") as f:\n",
    "            text = f.read()\n",
    "            if(filterComments):\n",
    "                text = bagOfWords.filterComments(text)\n",
    "            wordList = gensim.utils.simple_preprocess(text) #maybe add min_len and max_len params\n",
    "            corpus.append(wordList)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = getCorpus(2)\n",
    "        \n",
    "X = np.array(corpus)\n",
    "\n",
    "\n",
    "#get expected values\n",
    "predicted_class_names = ['Vulnerable']\n",
    "\n",
    "#X = data_frame[feature_col_names].values # predictor feature columns (17 x m)\n",
    "y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)\n",
    "\n",
    "#get a nonvulnerable column for y labels\n",
    "y_flipped = np.array(y, copy=True)\n",
    "for i in range(len(y_flipped)):\n",
    "    if y_flipped[i][0] == 0:\n",
    "        y_flipped[i][0] = 1\n",
    "    elif y_flipped[i][0] == 1:\n",
    "        y_flipped[i][0] = 0\n",
    "        \n",
    "y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner\n",
    "\n",
    "print(len(corpus))\n",
    "print(len(y))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6796,)\n",
      "(2913,)\n",
      "(6796, 2)\n",
      "(2913, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelize data and set dimensions\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "#LabeledSentence = gensim.models.doc2vec.TaggedDocument #before feeding token list to word2vec, they must be LabeledSentences\n",
    "\n",
    "def labelizeWords(words, label_type):\n",
    "    labelized = []\n",
    "    for i,v in enumerate(words):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        #labelized.append(LabeledSentence(v, [label]))\n",
    "        labelized.append(TaggedDocument(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "X_train = labelizeWords(X_train, 'TRAIN')\n",
    "X_test = labelizeWords(X_test, 'TEST')\n",
    "\n",
    "n_dim = 100 #100 b/c the GloVe is 100 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['php', 'class', 'input', 'private', 'input', 'public', 'function', 'getinput', 'return', 'this', 'input', 'realone', 'public', 'function', 'this', 'input', 'array', 'this', 'input', 'test', 'safe', 'this', 'input', 'realone', 'userdata', 'this', 'input', 'trap', 'safe', 'temp', 'new', 'input', 'tainted', 'temp', 'getinput', 'legal_table', 'array', 'safe', 'safe', 'if', 'in_array', 'tainted', 'legal_table', 'true', 'tainted', 'tainted', 'else', 'tainted', 'legal_table', 'query', 'select', 'from', 'tainted', 'conn', 'mysql_connect', 'localhost', 'mysql_user', 'mysql_password', 'mysql_select_db', 'dbname', 'echo', 'query', 'query', 'br', 'br', 'res', 'mysql_query', 'query', 'while', 'data', 'res', 'print_r', 'data', 'echo', 'br', 'mysql_close', 'conn'], tags=['TRAIN_0'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 472\n"
     ]
    }
   ],
   "source": [
    "#building tf-idf vectorizer\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([X.words for X in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given list of tokens, creates averaged vector. Used to turn X_train and X_test into vector list\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6796\n",
      "(6796, 100)\n",
      "(2913, 100)\n"
     ]
    }
   ],
   "source": [
    "#scale columns in X_train and X_test to \n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, X_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print(len(train_vecs_w2v))\n",
    "print(train_vecs_w2v.shape)\n",
    "print(test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-17 14:41:18,555 : WARNING : From C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-07-17 14:41:18,635 : WARNING : From C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      " - 0s - loss: 0.3101 - acc: 0.8956\n",
      "Epoch 2/9\n",
      " - 0s - loss: 0.2389 - acc: 0.9151\n",
      "Epoch 3/9\n",
      " - 0s - loss: 0.2102 - acc: 0.9264\n",
      "Epoch 4/9\n",
      " - 0s - loss: 0.1847 - acc: 0.9341\n",
      "Epoch 5/9\n",
      " - 0s - loss: 0.1675 - acc: 0.9403\n",
      "Epoch 6/9\n",
      " - 0s - loss: 0.1552 - acc: 0.9456\n",
      "Epoch 7/9\n",
      " - 0s - loss: 0.1464 - acc: 0.9509\n",
      "Epoch 8/9\n",
      " - 0s - loss: 0.1404 - acc: 0.9506\n",
      "Epoch 9/9\n",
      " - 0s - loss: 0.1361 - acc: 0.9538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25dc6908e48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building the network\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9483350506976344"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9602187e-01 2.0778477e-03]\n",
      " [9.9516165e-01 3.3304393e-03]\n",
      " [9.9472642e-01 6.5480769e-03]\n",
      " ...\n",
      " [2.3364145e-01 7.1951872e-01]\n",
      " [2.3538473e-01 7.6976937e-01]\n",
      " [9.9998784e-01 6.5812569e-06]]\n",
      "2913/2913 [==============================] - 0s 9us/step\n",
      "Precision: 0.7904411764705882\n",
      "Recall 0.7026143790849673\n",
      "F1-Score 0.7439446366782007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      2607\n",
      "           1       0.79      0.70      0.74       306\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2913\n",
      "   macro avg       0.88      0.84      0.86      2913\n",
      "weighted avg       0.95      0.95      0.95      2913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "#predict values using the testing data\n",
    "predict_test = model.predict(test_vecs_w2v)\n",
    "\n",
    "print(predict_test)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "'''precision = metrics.precision_score(y_test, predict_test, pos_label = 1)\n",
    "recall = metrics.recall_score(y_test, predict_test, pos_label = 1)\n",
    "f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)'''\n",
    "\n",
    "predicted = np.argmax(predict_test, axis=1)\n",
    "\n",
    "'''for prediction in predicted:\n",
    "    print(prediction)'''\n",
    "\n",
    "report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)\n",
    "#Accuracy\n",
    "test_loss, test_acc = model.evaluate(test_vecs_w2v, y_test)\n",
    "precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall\", recall)\n",
    "print(\"F1-Score\", f1_score)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
