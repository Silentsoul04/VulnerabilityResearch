{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import csvWrite\n",
    "\n",
    "#do plotting inline\n",
    "%matplotlib inline\n",
    "\n",
    "#get directory\n",
    "csvDirectory = csvWrite.getCSVDirectory()\n",
    "\n",
    "data_frame = pd.read_csv(csvDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col_names = csvWrite.getAllRowHeaders()\n",
    "feature_col_names = feature_col_names[2:] #all except name and vulnerable column\n",
    "predicted_class_names = ['Vulnerable']\n",
    "\n",
    "X = data_frame[feature_col_names].values # predictor feature columns (17 x m)\n",
    "y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(modelType):\n",
    "    #modelType is a string for the model being used\n",
    "    \n",
    "    # prepare the k-fold cross-validation configuration\n",
    "    n_folds = 10\n",
    "    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle\n",
    "\n",
    "    # cross validation estimation of performance\n",
    "    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        # select samples\n",
    "        trainX, trainy = X[train_ix], y[train_ix]\n",
    "        testX, testy = X[test_ix], y[test_ix]\n",
    "        # evaluate model\n",
    "        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)\n",
    "        \n",
    "        if modelType == \"nb\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_nb(trainX, trainy, testX, testy)\n",
    "        elif modelType == \"rf\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_rf(trainX, trainy, testX, testy)\n",
    "        elif modelType == \"lr\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_lr(trainX, trainy, testX, testy)\n",
    "        elif modelType == \"mlp\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(trainX, trainy, testX, testy)\n",
    "        elif modelType == \"lstm\":\n",
    "            # reshape input to be [samples, time steps, features]\n",
    "            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "            \n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_lstm(trainX, trainy, testX, testy)\n",
    "        #print('Accuracy >%.3f' % test_acc)\n",
    "        #print('Precision >%.3f' % precision)\n",
    "        #print('Recall >%.3f' % recall)\n",
    "        #print('F1-Scores >%.3f' % f1_score)\n",
    "        scores.append(test_acc)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "        members.append(model)\n",
    "\n",
    "    # summarize expected performance\n",
    "    print(modelType, \"Performance\")\n",
    "    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))\n",
    "    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))\n",
    "    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_nb(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create Gaussian Naive Bayes model object and train it with the data\n",
    "    model = GaussianNB()\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    #Accuracy\n",
    "    test_acc = metrics.accuracy_score(y_test, predict_test)\n",
    "    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)\n",
    "    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 10 Fold Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb Performance\n",
      "Estimated Accuracy: 0.905, Std Ddv: (0.011)\n",
      "Estimated Precision: 0.911, Std Ddv: (0.067)\n",
      "Estimated Recall: 0.133, Std Ddv: (0.030)\n",
      "Estimated F1-Score: 0.231, Std Ddv: (0.047)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(\"nb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_rf(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create Random Classifier model object and train it with the data\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    #Accuracy\n",
    "    test_acc = metrics.accuracy_score(y_test, predict_test)\n",
    "    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)\n",
    "    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 10 Fold Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf Performance\n",
      "Estimated Accuracy: 0.902, Std Ddv: (0.011)\n",
      "Estimated Precision: 0.692, Std Ddv: (0.166)\n",
      "Estimated Recall: 0.241, Std Ddv: (0.091)\n",
      "Estimated F1-Score: 0.337, Std Ddv: (0.084)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(\"rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_lr(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create Logistic Regression model object and train it with the data\n",
    "    model = LogisticRegression(C=0.7, random_state=42) #C is regularization hyperparameter\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    #Accuracy\n",
    "    test_acc = metrics.accuracy_score(y_test, predict_test)\n",
    "    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)\n",
    "    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr Performance\n",
      "Estimated Accuracy: 0.904, Std Ddv: (0.011)\n",
      "Estimated Precision: 0.926, Std Ddv: (0.047)\n",
      "Estimated Recall: 0.125, Std Ddv: (0.028)\n",
      "Estimated F1-Score: 0.219, Std Ddv: (0.044)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(\"lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Training Data for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_frame[feature_col_names].values # predictor feature columns (17 x m)\n",
    "y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)\n",
    "#get a nonvulnerable column for y labels\n",
    "y_flipped = np.array(y, copy=True)\n",
    "for i in range(len(y_flipped)):\n",
    "    if y_flipped[i][0] == 0:\n",
    "        y_flipped[i][0] = 1\n",
    "    elif y_flipped[i][0] == 1:\n",
    "        y_flipped[i][0] = 0\n",
    "        \n",
    "y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_mlp(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create MLP model object and train it with the data\n",
    "    model = keras.Sequential([\n",
    "    #start with layer of input shape (anything, 17)\n",
    "    keras.layers.Dense(500, input_shape=(csvWrite.getNumFeatures(),), activation=tf.nn.relu),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.sigmoid) #keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, y_train, epochs=5)\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    predicted = np.argmax(predict_test, axis=1)\n",
    "    \n",
    "    #print(np.argmax(y_test, axis=1).shape)\n",
    "    #print(np.argmax(y_test, axis=1))\n",
    "    #print(predict_test.shape)\n",
    "    #print(predict_test)\n",
    "    \n",
    "    #import the performance metrics library\n",
    "    report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)\n",
    "    #Accuracy\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "    recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ga6198\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (518,) but got array with shape (18,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c2abe467f0ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mlp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-dc8383109843>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(modelType)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmodelType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mlp\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model_mlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmodelType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lstm\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# reshape input to be [samples, time steps, features]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-44c929a9e994>\u001b[0m in \u001b[0;36mevaluate_model_mlp\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2382\u001b[1;33m         exception_prefix='input')\n\u001b[0m\u001b[0;32m   2383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[1;34m'Error when checking '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    363\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (518,) but got array with shape (18,)"
     ]
    }
   ],
   "source": [
    "cross_validation(\"mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_lstm(trainX, trainy, testX, testy):\n",
    "    dataDimensions = csvWrite.getNumFeatures()\n",
    "    \n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, dataDimensions)))\n",
    "    model.add(Dense(2)) #output layer, vulner or nonvulner\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainy, epochs=5, batch_size=1, verbose=2)\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(testX)\n",
    "    \n",
    "    predicted = np.argmax(predict_test, axis=1)\n",
    "\n",
    "    #metrics\n",
    "    report = metrics.classification_report(np.argmax(testy, axis=1), predicted)\n",
    "    #test_loss, test_acc = model.evaluate(testX, testy)\n",
    "    test_acc = metrics.accuracy_score(np.argmax(testy, axis=1), predicted)\n",
    "    precision = metrics.precision_score(np.argmax(testy, axis=1), predicted, pos_label = 1)\n",
    "    recall = metrics.recall_score(np.argmax(testy, axis=1), predicted, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(np.argmax(testy, axis=1), predicted, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(\"lstm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
