def chooseModelType_word2vec(X, y):
    print("Choose model type by entering number:")
    print("0: All Models")
    print("1: Naive-Bayes")
    print("2: Random Forest")
    print("3: Logistic Regression")
    print("4: Multilayer Perceptron")
    print("5: Long Short-Term Memory")
    user_input = int(input())
    print("Training classifier...")
    if user_input == 0:
        cross_validation_word2vec("nb", X, y)
        cross_validation_word2vec("rf", X, y)
        cross_validation_word2vec("lr", X, y)    
        cross_validation_word2vec("mlp", X, y)
        cross_validation_word2vec("lstm", X, y)
    elif user_input == 1:
        cross_validation_word2vec("nb", X, y)
    elif user_input == 2:
        cross_validation_word2vec("rf", X, y)
    elif user_input == 3:
        cross_validation_word2vec("lr", X, y)
    elif user_input == 4:   
        cross_validation_word2vec("mlp", X, y)
    elif user_input == 5:
        cross_validation_word2vec("lstm", X, y)
    else:
        print("Input not understood.")

    print("Training completed")

def cross_validation_word2vec(modelType, X, y):
    '''Cross validation for various model types. modelType is a string for the model being used'''
    # prepare the k-fold cross-validation configuration
    n_folds = 10
    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle

    # cross validation estimation of performance
    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()

    for train_ix, test_ix in kfold.split(X):
        # select samples
        trainX, trainy = X[train_ix], y[train_ix]
        testX, testy = X[test_ix], y[test_ix]

        #building word2vec model
        LabeledSentence = gensim.models.doc2vec.LabeledSentence #before feeding token list to word2vec, they must be LabeledSentences
            
        trainX = labelizeWords(trainX, 'TRAIN')
        testX = labelizeWords(testX, 'TEST')

        n_dim = 150

        model = gensim.models.Word2Vec(size=n_dim, window=10, min_count=10, workers=10)
        model.build_vocab([X.words for X in trainX])
        model.train([X.words for X in trainX], total_examples=len(trainX), epochs=5)


        #BUILDING THE CLASSIFIER
        #building tf-idf vectorizer
        print('building tf-idf matrix ...')
        vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
        matrix = vectorizer.fit_transform([X.words for X in trainX])
        tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
        print('vocab size :', len(tfidf))

        #scale columns in X_train and X_test
        train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, trainX)])
        train_vecs_w2v = scale(train_vecs_w2v)

        test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, testX)])
        test_vecs_w2v = scale(test_vecs_w2v)
        
        # evaluate model
        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)
        
        if modelType == "nb":
            model, test_acc, precision, recall, f1_score = evaluate_model_nb(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        elif modelType == "rf":
            model, test_acc, precision, recall, f1_score = evaluate_model_rf(train_vecs_w2v, trainy, test_vecs_w2v, testy)
            print(model.feature_importances_)
        elif modelType == "lr":
            model, test_acc, precision, recall, f1_score = evaluate_model_lr(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        elif modelType == "mlp":
            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        elif modelType == "lstm":
            # reshape input to be [samples, time steps, features]
            train_vecs_w2v = np.reshape(train_vecs_w2v, (train_vecs_w2v.shape[0], 1, train_vecs_w2v.shape[1]))
            test_vecs_w2v = np.reshape(test_vecs_w2v, (test_vecs_w2v.shape[0], 1, test_vecs_w2v.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_lstm(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        scores.append(test_acc)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1_score)
        members.append(model)

    # summarize expected performance
    print(modelType, "Performance")
    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))
    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))
    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))
    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))
