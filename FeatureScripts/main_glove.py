from file import File
import os

#data containers
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#classifier models/tools
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC  
from sklearn import tree
from sklearn.model_selection import KFold # import KFold
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import scale
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectKBest #for feature selection
from sklearn.feature_selection import chi2

import tensorflow as tf
from tensorflow import keras
#import keras
#from keras.models import load_model
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D
from keras.layers import LSTM
from keras.layers import SimpleRNN
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from numpy import array
from numpy import asarray
from numpy import zeros

#for getting data from csv
import csvWrite
import fileRetriever
import bagOfWords

import math

#for word2vec
import gensim

#saving/loading models
import pickle

#returns a list of strings. Each string is a file's text
def getCorpus():
    files = fileRetriever.getFileSet(2)
    corpus = []
    for i, file in enumerate(files):
        wordList = []
        with open(file.dir, "r", encoding="utf8") as f:
            text = f.read()
            text = bagOfWords.filterComments(text)
            #wordList = gensim.utils.simple_preprocess(text) #maybe add min_len and max_len params
            #corpus.append(wordList)
            corpus.append(text) # add the string to the corpus

    return corpus

#PROGRAM START
print("Welcome to the SQL injection machine learning classifier!")
loop = True

while loop:
    #printMenu()
    choice = int(input("Enter your choice: "))

    if choice == 1:
        print("Will retrain classifier")

        print("Rewrite fileInfo.csv? Contains feature information for file set ")
        csvWrite.rewriteCSV()

        #get data frame of features
        csvDirectory = csvWrite.getCSVDirectory()
        data_frame = pd.read_csv(csvDirectory)    
        
        featureSetChoice = int(input("Train bagofwords (0), word2vec (1), or glove (2) implementation? "))
        if featureSetChoice == 0:

            #divide training and testing set
            feature_col_names, predicted_class_names = csvWrite.getFeatureColumns()
            print("feature col names before:", feature_col_names, "length:", len(feature_col_names))

            #data frame filtering
            data_frame, feature_col_names = removeFeatures(data_frame, feature_col_names)
            print("feature col names after:", feature_col_names, "length:", len(feature_col_names))

            #establish X and y data sets
            X = data_frame[feature_col_names].values # predictor feature columns (~500 x m)
            y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

            print("Number files in X:", len(X))
            print("Number features in X:", len(X[0]))

            #print most important features
            printBestFeatures(feature_col_names)

            print("Starting classifier training")
            chooseModelType(X, y)
            
        elif featureSetChoice == 2:
            #####Loading Glove Model#####
            '''from gensim.scripts.glove2word2vec import glove2word2vec
            glove_input_file = 'glove.6B.100d.txt'
            word2vec_output_file = 'glove.6B.100d.txt.word2vec'
            if not os.path.exists(word2vec_output_file):
                print("glove.6b.100d.txt.word2vec does not exist. Creating it now")
                glove2word2vec(glove_input_file, word2vec_output_file)

            from gensim.models import KeyedVectors
            # load the Stanford GloVe model. I think this is already trained
            filename = 'glove.6B.100d.txt.word2vec'
            model = KeyedVectors.load_word2vec_format(filename, binary=False)'''

            #get corpus
            print("Retrieving corpus...")
            corpus = getCorpus()

            #####Establish X and y data sets#####
            #X = np.array(corpus)
            X = corpus
            #get expected values
            predicted_class_names = ['Vulnerable']

            #X = data_frame[feature_col_names].values # predictor feature columns (17 x m)
            y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

            #get a nonvulnerable column for y labels
            '''y_flipped = np.array(y, copy=True)
            for i in range(len(y_flipped)):
                if y_flipped[i][0] == 0:
                    y_flipped[i][0] = 1
                elif y_flipped[i][0] == 1:
                    y_flipped[i][0] = 0
                    
            y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner
            print("Starting classifier training")
            chooseModelType_word2vec(X, y)'''

            t = Tokenizer()
            #creates vocabulary based on words in X (The tokens in all files)
            t.fit_on_texts(X)
            vocab_size = len(t.word_index) + 1
            #integer encode the documents (turn the words into sequences of ints)
            encoded_docs = t.texts_to_sequences(X)
            print(encoded_docs)
            #pad documents to a numpy array, with everything the same length
            max_length = 50
            padded_docs = pad_sequences(encoded_docs, padding='post', maxlen=max_length)
            print(padded_docs)

            #load entire GloVe embedding into memory
            target_dimension = 100
            
            print("loading GloVe embedding...")
            embeddings_index = dict()
            f = open('glove.6B.100d.txt', encoding='utf8')
            for line in f:
                values = line.split()
                word = values[0]
                coefs = asarray(values[1:], dtype='float32')
                embeddings_index[word] = coefs
            f.close()
            print('Loaded %s word vectors.' % len(embeddings_index))

            #create a weight matrix for each word in the training dataset
            embedding_matrix = zeros((vocab_size, target_dimension)) #x-axis: number of words, y-axis: number of dimensions
            for word, i, in t.word_index.items():
                embedding_vector = embeddings_index.get(word)
                if embedding_vector is not None:
                    embedding_matrix[i] = embedding_vector

            #define model
            model = Sequential()
            #may need to add input_length, since there is a Flatten() and Dense layer
            e = Embedding(vocab_size, target_dimension, weights=[embedding_matrix], input_length=max_length)
            model.add(e)
            model.add(Flatten())
            model.add(Dense(500, activation='relu'))
            model.add(Dense(16, activation='relu'))
            model.add(Dense(1, activation='sigmoid'))

            #compile the model
            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

            #fit model
            #switch epochs to 10
            history = model.fit(padded_docs, y,epochs=50)

            #evaluate the model
            loss, accuracy = model.evaluate(padded_docs, y, verbose=0)
            print('Accuracy: %f' % (accuracy*100))

    elif choice == 3:
        print("Quitting program")
        loop = False
    else:
        print("Input not understood. Please try again")

