from file import File
import os

#data containers
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#classifier models/tools
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC  
from sklearn import tree
from sklearn.model_selection import KFold # import KFold
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import scale
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectKBest #for feature selection
from sklearn.feature_selection import chi2

import tensorflow as tf
from tensorflow import keras
#import keras
#from keras.models import load_model
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D
from keras.layers import LSTM
from keras.layers import SimpleRNN
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from numpy import array
from numpy import asarray
from numpy import zeros

#for getting data from csv
import csvWrite
import fileRetriever
import bagOfWords

import math

#for word2vec
import gensim

#saving/loading models
import pickle

#garbage collection
import gc


#free up memory
gc.collect()

def cross_validation(modelType, X, y):
    '''Cross validation for various model types. modelType is a string for the model being used'''
    # prepare the k-fold cross-validation configuration
    n_folds = 10
    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle

    # cross validation estimation of performance
    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()

    counter = 1
    for train_ix, test_ix in kfold.split(X):
        print("Cross validation iteration", counter)
        counter = counter + 1
        
        # select samples
        trainX, trainy = X[train_ix], y[train_ix]
        testX, testy = X[test_ix], y[test_ix]
        # evaluate model
        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)

        print("trainX shape:", trainX.shape)
        print("testX shape:", testX.shape)
        print("trainy shape:", trainy.shape)
        print("testy shape:", testy.shape)
        
        if modelType == "mlp":
            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(trainX, trainy, testX, testy, "bagOfWords")
        
        scores.append(test_acc)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1_score)
        members.append(model)
        
        #free up memory
        gc.collect()

    # summarize expected performance
    print(modelType, "Performance")
    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))
    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))
    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))
    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))

def evaluate_model_mlp(X_train, y_train, X_test, y_test, featureType):
    # define model
    #create MLP model object and train it with the data

    ####preprocess X train####
    t = Tokenizer()
    #creates vocabulary based on words in X (The tokens in all files)
    t.fit_on_texts(X_train)
    vocab_size = len(t.word_index) + 1
    #integer encode the documents (turn the words into sequences of ints)
    print("Converting documents words into sequence of integers")
    encoded_docs = t.texts_to_sequences(X_train)
    print(encoded_docs)
    #pad documents to a numpy array, with everything the same length
    print("Make the inputs the same length")
    max_length = 50
    padded_docs = pad_sequences(encoded_docs, padding='post', maxlen=max_length)
    print(padded_docs)
    #load entire GloVe embedding into memory
    target_dimension = 100
            
    print("loading GloVe embedding...")
    embeddings_index = dict()
    f = open('glove.6B.100d.txt', encoding='utf8')
    for line in f:
        values = line.split()
        word = values[0]
        coefs = asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()
    print('Loaded %s word vectors.' % len(embeddings_index))

    #create a weight matrix for each word in the training dataset
    embedding_matrix = zeros((vocab_size, target_dimension)) #x-axis: number of words, y-axis: number of dimensions
    for word, i, in t.word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

    #define model
    model = Sequential()
    #may need to add input_length, since there is a Flatten() and Dense layer
    e = Embedding(vocab_size, target_dimension, weights=[embedding_matrix], input_length=max_length)
    model.add(e)
    model.add(Flatten())
    model.add(Dense(500, activation='relu'))
    model.add(Dense(16, activation='relu'))
    #model.add(Dense(1, activation='sigmoid'))
    model.add(Dense(2, activation='sigmoid'))

    #compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    #fit model
    #switch epochs to 10
    history = model.fit(padded_docs, y_train, epochs=10)

    #evaluate the model
    loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)
    print('Accuracy: %f' % (accuracy*100))
    
    model.compile(optimizer='adam', 
              loss='binary_crossentropy',
              metrics=['accuracy'])
    
    # fit model
    history = model.fit(padded_docs, y_train, epochs=10)

    # save model
    '''filename = 'mlp.sav'
    pickle.dump(model, open(filename, 'wb'))'''
    '''if featureType == "bagOfWords":
        filename = 'mlp.h5'
        model.save(filename)
    elif featureType == "word2vec":
        filename = 'mlp_word2vec.h5'
        model.save(filename)
    else:
        print("Classifier not saved")'''

    ####preprocess X test####
    #t2 = Tokenizer()
    #creates vocabulary based on words in X (The tokens in all files)
    #t2.fit_on_texts(X_test)
    #vocab_size = len(t2.word_index) + 1
    #integer encode the documents (turn the words into sequences of ints)
    print("Converting documents words into sequence of integers")
    #encoded_docs_test = t2.texts_to_sequences(X_test)
    encoded_docs_test = t.texts_to_sequences(X_test)
    print(encoded_docs_test)
    #pad documents to a numpy array, with everything the same length
    print("Make the inputs the same length")
    padded_docs_test = pad_sequences(encoded_docs_test, padding='post', maxlen=max_length)
    print(padded_docs_test)
    
    #evaluate the model
    #predict values using the testing data
    predict_test = model.predict(padded_docs_test) #model.predict(X_test)

    predicted = np.argmax(predict_test, axis=1)
    
    print(y_test)
    print(predict_test)
    print(y_test.shape)
    print(predict_test.shape)
    print(X_train.shape)
    print(X_test.shape)
    
    report = metrics.classification_report(np.argmax(y_test, axis=1), predicted) #report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)

    #Accuracy
    try:
        test_loss, test_acc = model.evaluate(padded_docs_test, y_test)
    except Exception as inst:
        print(inst)
    #test_acc = metrics.accuracy_score(y_test, predict_test)
    #precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    #recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    #f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score


#returns a list of strings. Each string is a file's text
def getCorpus():
    files = fileRetriever.getFileSet(2)
    corpus = []
    for i, file in enumerate(files):
        wordList = []
        with open(file.dir, "r", encoding="utf8") as f:
            text = f.read()
            text = bagOfWords.filterComments(text)
            #wordList = gensim.utils.simple_preprocess(text) #maybe add min_len and max_len params
            #corpus.append(wordList)
            corpus.append(text) # add the string to the corpus

    return corpus

#PROGRAM START
print("Welcome to the SQL injection machine learning classifier!")
loop = True

while loop:
    #printMenu()
    choice = int(input("Enter your choice: "))

    if choice == 1:
        print("Will retrain classifier")

        print("Rewrite fileInfo.csv? Contains feature information for file set ")
        csvWrite.rewriteCSV()

        #get data frame of features
        csvDirectory = csvWrite.getCSVDirectory()
        data_frame = pd.read_csv(csvDirectory)    
        
        featureSetChoice = int(input("Train bagofwords (0), word2vec (1), or glove (2) implementation? "))
        if featureSetChoice == 0:

            #divide training and testing set
            feature_col_names, predicted_class_names = csvWrite.getFeatureColumns()
            print("feature col names before:", feature_col_names, "length:", len(feature_col_names))

            #data frame filtering
            data_frame, feature_col_names = removeFeatures(data_frame, feature_col_names)
            print("feature col names after:", feature_col_names, "length:", len(feature_col_names))

            #establish X and y data sets
            X = data_frame[feature_col_names].values # predictor feature columns (~500 x m)
            y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

            print("Number files in X:", len(X))
            print("Number features in X:", len(X[0]))

            #print most important features
            printBestFeatures(feature_col_names)

            print("Starting classifier training")
            chooseModelType(X, y)
            
        elif featureSetChoice == 2:
            #####Loading Glove Model#####
            '''from gensim.scripts.glove2word2vec import glove2word2vec
            glove_input_file = 'glove.6B.100d.txt'
            word2vec_output_file = 'glove.6B.100d.txt.word2vec'
            if not os.path.exists(word2vec_output_file):
                print("glove.6b.100d.txt.word2vec does not exist. Creating it now")
                glove2word2vec(glove_input_file, word2vec_output_file)

            from gensim.models import KeyedVectors
            # load the Stanford GloVe model. I think this is already trained
            filename = 'glove.6B.100d.txt.word2vec'
            model = KeyedVectors.load_word2vec_format(filename, binary=False)'''

            #get corpus
            print("Retrieving corpus...")
            corpus = getCorpus()

            #####Establish X and y data sets#####
            X = np.array(corpus)
            #X = corpus
            #get expected values
            predicted_class_names = ['Vulnerable']

            #X = data_frame[feature_col_names].values # predictor feature columns (17 x m)
            y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

            #get a nonvulnerable column for y labels
            y_flipped = np.array(y, copy=True)
            for i in range(len(y_flipped)):
                if y_flipped[i][0] == 0:
                    y_flipped[i][0] = 1
                elif y_flipped[i][0] == 1:
                    y_flipped[i][0] = 0
                    
            y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner
            print("Starting classifier training")

            cross_validation("mlp", X, y)

    elif choice == 3:
        print("Quitting program")
        loop = False
    else:
        print("Input not understood. Please try again")

