#####Loading Glove Model#####
from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'glove.6B.100d.txt'
word2vec_output_file = 'glove.6B.100d.txt.word2vec'
glove2word2vec(glove_input_file, word2vec_output_file)

from gensim.models import KeyedVectors
# load the Stanford GloVe model. I think this is already trained
filename = 'glove.6B.100d.txt.word2vec'
model = KeyedVectors.load_word2vec_format(filename, binary=False)
# calculate: (king - man) + woman = ?
result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
print(result)


#####Establishing Data Set#####
import gensim 
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

assert gensim.models.doc2vec.FAST_VERSION > -1 #ensure fast performance. Need to download C compiler, like MinGW

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold # import KFold
from sklearn import metrics
#import tensorflow as tf
#from tensorflow import keras
from sklearn.feature_extraction.text import TfidfVectorizer
from keras.models import Sequential
from keras.layers import Dense

from sklearn.feature_selection import SelectKBest #for feature selection
from sklearn.feature_selection import chi2

import csvWrite
import fileRetriever
import bagOfWords

#Get data frame
#get directory
csvDirectory = csvWrite.getCSVDirectory()

data_frame = pd.read_csv(csvDirectory)

#get corpus
def getCorpus(fileSetNum, filterComments=True):
    files = fileRetriever.getFileSet(fileSetNum)
    corpus = []
    for i, file in enumerate(files):
        wordList = []
        with open(file.dir, "r", encoding="utf8") as f:
            text = f.read()
            if(filterComments):
                text = bagOfWords.filterComments(text)
            wordList = gensim.utils.simple_preprocess(text) #maybe add min_len and max_len params
            corpus.append(wordList)

    return corpus

corpus = []
userInput = int(input("Juliet Files (2) or Full File Set (3)?"))
if userInput == 2:
    corpus = getCorpus(2)
elif userInput == 3:
    corpus = getcorpus(3)
else:
    print("Input not understood. Using Juliet Files")
    corpus = getCorpus(2)
        
X = np.array(corpus)


#get expected values
predicted_class_names = ['Vulnerable']

#X = data_frame[feature_col_names].values # predictor feature columns (17 x m)
y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

#get a nonvulnerable column for y labels
y_flipped = np.array(y, copy=True)
for i in range(len(y_flipped)):
    if y_flipped[i][0] == 0:
        y_flipped[i][0] = 1
    elif y_flipped[i][0] == 1:
        y_flipped[i][0] = 0
        
y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner

print(len(corpus))
print(len(y))
print(len(X))

#####Split Data Set#####
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#labelize data and set dimensions
from gensim.models.doc2vec import TaggedDocument
#LabeledSentence = gensim.models.doc2vec.TaggedDocument #before feeding token list to word2vec, they must be LabeledSentences

def labelizeWords(words, label_type):
    labelized = []
    for i,v in enumerate(words):
        label = '%s_%s'%(label_type,i)
        #labelized.append(LabeledSentence(v, [label]))
        labelized.append(TaggedDocument(v, [label]))
    return labelized

X_train = labelizeWords(X_train, 'TRAIN')
X_test = labelizeWords(X_test, 'TEST')

n_dim = 100 #100 b/c the GloVe is 100 dim

#####Building the Classifier#####
#building tf-idf vectorizer
print('building tf-idf matrix ...')
vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
matrix = vectorizer.fit_transform([X.words for X in X_train])
tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
print('vocab size :', len(tfidf))

#given list of tokens, creates averaged vector. Used to turn X_train and X_test into vector list
def buildWordVector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += model[word].reshape((1, size)) * tfidf[word]
            count += 1.
        except KeyError: # handling the case where the token is not
                         # in the corpus. useful for testing.
            continue
    if count != 0:
        vec /= count
    return vec

#scale columns in X_train and X_test to 

from sklearn.preprocessing import scale
train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, X_train)])
train_vecs_w2v = scale(train_vecs_w2v)

test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x.words, X_test)])
test_vecs_w2v = scale(test_vecs_w2v)

print(len(train_vecs_w2v))
print(train_vecs_w2v.shape)
print(test_vecs_w2v.shape)

#building the network
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=n_dim))
model.add(Dense(2, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)

test_loss, test_acc = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)

# evaluate the model
#predict values using the testing data
predict_test = model.predict(test_vecs_w2v)

print(predict_test)

test_loss, test_acc = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)

predicted = np.argmax(predict_test, axis=1)

report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)
#Accuracy
test_loss, test_acc = model.evaluate(test_vecs_w2v, y_test)
precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)

print("Precision:", precision)
print("Recall", recall)
print("F1-Score", f1_score)

print(report)
