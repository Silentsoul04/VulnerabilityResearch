import fileRetriever
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk

import re

'''def getBagOfWords(fileSetChoice):
    files = fileRetriever.getFileSet(fileSetChoice)
    textList = [] #list of text for all files
    for file in files:
        with open(file.dir, "r") as f:
            lines = list(f) # store all lines together
            text = "".join(lines) # join lines into single string
            textList.append(text)

    #initialize vectorizer
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(textList) #tokenize and count words
    print(X.toarray())
    return X.toarray()'''

'''def tokenizeCode(text):
    tokens = nltk.word_tokenize(text)
    return tokens'''

def getStopWordsList(fileList, useFunctionNames=False):
    '''get stop words, which are variable and function names'''
    #varPattern = re.compile("[$][a-zA-Z][a-zA-Z\d]*")
    #funcPattern = re.compile("[a-zA-Z][a-zA-Z\d]*[(].*[)]")
    varPattern = r"[$][a-zA-Z][a-zA-Z\d]*"
    funcPattern = r"[a-zA-Z][a-zA-Z\d]*[(]"

    varList = set()
    funcList = set()

    textList = getTextList(fileList)
    for text in textList:
        varMatches = re.search(varPattern, text)
        if varMatches:
            #print("Variable matches")
            varList.add(varMatches.group(0))

    varList = list(varList)
    #remove $ from values. ex. turn $action to action
    for i, var in enumerate(varList):
        varList[i] = varList[i][1:]
    print("Variable Stop Words")
    print(varList)

    if not useFunctionNames:
        return varList

    if useFunctionNames:
        for text in textList:
            funcMatches = re.search(funcPattern, text)
            if funcMatches:
                funcList.add(funcMatches.group(0))

        funcList = list(funcList)

        #remove ( from values. ex. turn action( to action
        for i, func in enumerate(funcList):
            funcList[i] = funcList[i][:-1]
        print("Function Stop Words")
        print(funcList)

        stopWordsList = varList + funcList
        return stopWordsList

def getTextList(fileList):
    '''get all text in files and put it in list'''
    textList = []
    for file in fileList:
        with open(file.dir, "r", encoding="utf8") as f:
            lines = list(f) # store all lines together
            text = "".join(lines) # join lines into single string
            textList.append(text)

    #print(textList)
    return textList

def getBagOfWords(fileList): #get bag of words w/ countVectorizer
    textList = getTextList(fileList)

    #print("Type of the textList", type(textList))

    #initialize vectorizer
    #vectorizer = CountVectorizer(tokenizer=tokenizeCode)
    vectorizer = CountVectorizer(min_df=10, max_features=500)
    X = vectorizer.fit_transform(textList) #tokenize and count words
    featureNames = vectorizer.get_feature_names()
    featureList = X.toarray()
    print(featureList)
    print("Shape", X.shape)
    print(featureNames)
    return featureNames, featureList #featureNames: [], featureList: [[]] 

def getBagOfWords_tdidf(fileList): #get bag of words w/ tdidfVectorizer
    textList = getTextList(fileList)

    #get stop words
    #stopWordsList = getStopWordsList(fileList)
    stopWordsList = getStopWordsList(fileList, useFunctionNames=True)

    #initialize vectorizer
    #vectorizer = TfidfVectorizer(min_df=10, max_features=500) #features w/ count of 5, top 500 features
    vectorizer = TfidfVectorizer(min_df=10, max_features=500, stop_words=stopWordsList)
    X = vectorizer.fit_transform(textList) #tokenize and count words
    featureNames = vectorizer.get_feature_names()
    featureList = X.toarray()
    print(featureList)
    print("Shape", X.shape)
    print(featureNames)
    return featureNames, featureList #featureNames: [], featureList: [[]]

if __name__ == "__main__":
    files = fileRetriever.getFileSet(0)
    getBagOfWords_tdidf(files)

    #getStopWordsList(files)
    #getStopWordsList(files, useFunctionNames = True)
