import fileRetriever
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
import pickle

import re

'''def getBagOfWords(fileSetChoice):
    files = fileRetriever.getFileSet(fileSetChoice)
    textList = [] #list of text for all files
    for file in files:
        with open(file.dir, "r") as f:
            lines = list(f) # store all lines together
            text = "".join(lines) # join lines into single string
            textList.append(text)

    #initialize vectorizer
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(textList) #tokenize and count words
    print(X.toarray())
    return X.toarray()'''

'''def tokenizeCode(text):
    tokens = nltk.word_tokenize(text)
    return tokens'''

def getStopWordsList(fileList, useFunctionNames=False):
    '''get stop words, which are variable and function names'''
    #varPattern = re.compile("[$][a-zA-Z][a-zA-Z\d]*")
    #funcPattern = re.compile("[a-zA-Z][a-zA-Z\d]*[(].*[)]")
    varPattern = r"[$][a-zA-Z][a-zA-Z\d]*"
    funcPattern = r"[a-zA-Z][a-zA-Z\d]*[(]"

    varList = set()
    funcList = set()

    textList = getTextList(fileList)
    for text in textList:
        varMatches = re.search(varPattern, text)
        if varMatches:
            #print("Variable matches")
            varList.add(varMatches.group(0))

    varList = list(varList)
    #remove $ from values. ex. turn $action to action
    for i, var in enumerate(varList):
        varList[i] = varList[i][1:]
    print("Variable Stop Words")
    print(varList)

    if not useFunctionNames:
        return varList
    elif useFunctionNames:
        for text in textList:
            funcMatches = re.search(funcPattern, text)
            if funcMatches:
                funcList.add(funcMatches.group(0))

        funcList = list(funcList)

        #remove ( from values. ex. turn action( to action
        for i, func in enumerate(funcList):
            funcList[i] = funcList[i][:-1]
        print("Function Stop Words")
        print(funcList)

        stopWordsList = varList + funcList
        return stopWordsList

def filterComments(text):
    '''filters out comments in a string'''
    #https://stackoverflow.com/questions/8748313/is-there-a-way-to-delete-all-comments-in-a-file-using-notepad
    #use this regex pattern?: \/\*.*?\*\/
    #test with application_top.php file
    #call in getTextList. text = filterComments(text)
    #https://stackoverflow.com/questions/13114104/matching-all-three-kinds-of-php-comments-with-regex
    commentPattern = r"(?:#|//)[^\r\n]*|/\*[\s\S]*?\*/"
    commentMatches = re.findall(commentPattern, text)
    #print(commentMatches)
    filteredString = text
    for match in commentMatches:
        filteredString = filteredString.replace(match, '')
    #print(filteredString)
    return filteredString

def getTextList(fileList):
    '''get all text in files and put it in list'''
    textList = []
    for file in fileList:
        with open(file.dir, "r", encoding="utf8") as f:
            lines = list(f) # store all lines together
            text = "".join(lines) # join lines into single string
            text = filterComments(text)
            textList.append(text)

    #print(textList)
    return textList

def getFunctionTextList(fileList):
    #get all function names in files and put it in list
    funcPattern = r"[a-zA-Z][a-zA-Z\d]*[(]"

    textList = getTextList(fileList)
    funcList = []

    for i in range(len(textList)):
        text = textList[i]
        funcMatches = re.findall(funcPattern, text)
        if funcMatches:
            text = funcMatches
            funcString = "" #will add all function names as a string, separated by spaces
            for func in text:
                funcWithoutParen = func[:-1]
                funcString = funcString + funcWithoutParen + " "
            funcList.append(funcString)
        else:
            funcList.append("")

    print(funcList)
    return funcList

'''def getFunctionTextList(fileList):
    #get all function names in files and put it in list
    funcPattern = r"[a-zA-Z][a-zA-Z\d]*[(]"

    funcList = []
    for file in fileList:
        with open(file.dir, "r", encoding="utf8") as f:
            lines = list(f) # store all lines together
            text = "".join(lines) # join lines into single string

            funcMatches = re.findall(funcPattern, text)
            if funcMatches:
                text = funcMatches
                funcString = "" #will add all function names as a string, separated by spaces
                for func in text:
                    funcWithoutParen = func[:-1]
                    funcString = funcString + funcWithoutParen + " "
                funcList.append(funcString)
            else:
                funcList.append("")

    print(funcList)
    return funcList'''

def getBagOfWords(fileList): #get bag of words w/ countVectorizer
    textList = getTextList(fileList)

    #print("Type of the textList", type(textList))

    #initialize vectorizer
    #vectorizer = CountVectorizer(tokenizer=tokenizeCode)
    vectorizer = CountVectorizer(min_df=10, max_features=500)
    X = vectorizer.fit_transform(textList) #tokenize and count words
    featureNames = vectorizer.get_feature_names()
    featureList = X.toarray()
    print(featureList)
    print("Shape", X.shape)
    print(featureNames)
    return featureNames, featureList #featureNames: [], featureList: [[]] 

def getBagOfWords_tfidf(fileList): #get bag of words w/ tfidfVectorizer
    textList = getTextList(fileList)

    #get stop words
    stopWordsList = getStopWordsList(fileList, useFunctionNames=False)
    #stopWordsList = getStopWordsList(fileList, useFunctionNames=True)

    #initialize vectorizer
    #vectorizer = TfidfVectorizer(min_df=10, max_features=500) #features w/ count of 5, top 500 features
    vectorizer = TfidfVectorizer(min_df=10, max_features=500, stop_words=stopWordsList)
    X = vectorizer.fit_transform(textList) #tokenize and count words

    #save vectorizer.vocabulary_
    pickle.dump(vectorizer.vocabulary_,open("feature_tfidf.pkl","wb"))
    
    featureNames = vectorizer.get_feature_names()
    featureList = X.toarray()
    #print(featureList)
    #print("Vectorizer Shape", X.shape)
    #print(featureNames)
    return featureNames, featureList #featureNames: [], featureList: [[]]

def getBagOfWords_functions(fileList): #get bag of words w/ tfidfVectorizer
    textList = getFunctionTextList(fileList)

    #get stop words
    stopWordsList = getStopWordsList(fileList, useFunctionNames=False)
    #stopWordsList = getStopWordsList(fileList, useFunctionNames=True)

    #initialize vectorizer
    #vectorizer = TfidfVectorizer(min_df=10, max_features=500) #features w/ count of 5, top 500 features
    vectorizer = TfidfVectorizer(min_df=10, max_features=500, stop_words=stopWordsList)
    X = vectorizer.fit_transform(textList) #tokenize and count words

    #save vectorizer.vocabulary_
    pickle.dump(vectorizer.vocabulary_,open("feature_functions.pkl","wb"))
    
    featureNames = vectorizer.get_feature_names()
    featureList = X.toarray()
    print(featureList)
    print("Vectorizer Shape", X.shape)
    print(featureNames)
    return featureNames, featureList #featureNames: [], featureList: [[]]

def useBagOfWords_tfidf(fileList):
    #reuse the tfidf vocabulary when entering files for testing

    textList = getTextList(fileList)
    #get stop words
    stopWordsList = getStopWordsList(fileList, useFunctionNames=False)

    #loaded_vec = TfidfVectorizer(min_df=10, max_features=500, vocabulary=pickle.load(open("feature_tfidf.pkl", "rb")), stop_words=stopWordsList)
    loaded_vec = TfidfVectorizer(min_df=10, max_features=500, vocabulary=pickle.load(open("feature_tfidf.pkl", "rb")))

    X = loaded_vec.fit_transform(textList) #X = loaded_vec.fit_transform(textList)

    featureNames = loaded_vec.get_feature_names()
    featureList = X.toarray()
    #print(featureList)
    #print("Vectorizer Shape", X.shape)
    #print(featureNames)
    return featureNames, featureList #featureNames: [], featureList: [[]]

if __name__ == "__main__":
    files = fileRetriever.getFileSet(2)
    #getBagOfWords_tfidf(files)

    #getStopWordsList(files)
    #getStopWordsList(files, useFunctionNames = True)

    #print(getTextList(files))

    #getFunctionTextList(files)
    #getBagOfWords_functions(files)

    getBagOfWords_tfidf(files)
    useBagOfWords_tfidf(files)
