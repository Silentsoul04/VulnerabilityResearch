import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import csvWrite

if __name__ == "__main__":
    #rewrite csv file?
    csvWrite.rewriteCSV()

    #create data frame
    csvDirectory = csvWrite.getCSVDirectory()

    data_frame = pd.read_csv(csvDirectory)

    '''#analyze data frame
    print("Data shape")
    data_frame.shape

    #look at first five rows
    print("First five rows")
    data_frame.head(5)

    #Check true/false ratio
    num_true = len(data_frame.loc[data_frame['Vulnerable'] == 1])
    num_false = len(data_frame.loc[data_frame['Vulnerable'] == 0])
    print("Num true cases: {0} ({1:2.2f}%)".format(num_true, (num_true/(num_true + num_false))*100))
    print("Num false cases: {0} ({1:2.2f}%)".format(num_false, (num_false/(num_true + num_false))*100))'''

    #Split data. 70% for training, 30% for testing
    from sklearn.model_selection import train_test_split

    feature_col_names = csvWrite.getRowHeaders()
    feature_col_names = feature_col_names[2:] #all except name and vulnerable column
    predicted_class_names = ['Vulnerable']

    X = data_frame[feature_col_names].values # predictor feature columns (17 x m)
    y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)
    split_test_size = 0.3 # percent of files that are in test set

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42)

    '''#check split
    print("{0:0.2f}% in training set".format((len(X_train)/len(data_frame.index))*100))
    print("{0:0.2f}% in training set".format((len(X_test)/len(data_frame.index))*100))

    #check split true/false ratio
    print("Training True: {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train) * 100)))
    print("Training False: {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train) * 100)))
    print()
    print("Test True: {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test) * 100)))
    print("Test False: {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test) * 100)))'''

    #Naive Bayes
    from sklearn.naive_bayes import GaussianNB

    #create Gaussian Naive Bayes model object and train it with the data
    print("Gaussian Naive Bayes")
    nb_model = GaussianNB()

    nb_model.fit(X_train, y_train.ravel())

    #predict values using the training data
    nb_predict_train = nb_model.predict(X_train)

    #import the performance metrics library
    from sklearn import metrics

    #Accuracy
    print("Training Accuracy: {0:.4f}".format(metrics.accuracy_score(y_train, nb_predict_train)))

    #predict values using the testing data
    nb_predict_test = nb_model.predict(X_test)

    #import the performance metrics library
    from sklearn import metrics

    #Accuracy
    print("Testing Accuracy: {0:.4f}".format(metrics.accuracy_score(y_test, nb_predict_test)))

    print("Confusion Matrix")
    print("{0}".format(metrics.confusion_matrix(y_test, nb_predict_test)))
    #left column: predicted false    right column: predicted true
    #top row: actual false    #bottom row: actual true
    print()
    print("Classification Report")
    print(metrics.classification_report(y_test, nb_predict_test))

    #Random Forest
    from sklearn.ensemble import RandomForestClassifier
    print("Random Forest")
    rf_model = RandomForestClassifier(random_state=42) #create random forest object
    rf_model.fit(X_train, y_train.ravel())

    #predict training data
    rf_predict_train = rf_model.predict(X_train)
    #training metrics
    print("Training Accuracy: {0:.4f}".format(metrics.accuracy_score(y_train, rf_predict_train)))

    #predict testing metrics
    rf_predict_test = rf_model.predict(X_test)
    #testing metrics
    print("Testing Accuracy: {0:.4f}".format(metrics.accuracy_score(y_test, rf_predict_test)))

    print("Confusion Matrix")
    print("{0}".format(metrics.confusion_matrix(y_test, rf_predict_test)))
    #left column: predicted false    right column: predicted true
    #top row: actual false    #bottom row: actual true
    print()
    print("Classification Report")
    print(metrics.classification_report(y_test, rf_predict_test))

    #Logistic Regression
    from sklearn.linear_model import LogisticRegression
    print("Logistic Regression")
    lr_model = LogisticRegression(C=0.7, random_state=42) #C is regularization hyperparameter
    lr_model.fit(X_train, y_train.ravel())
    lr_predict_test = lr_model.predict(X_test)

    #training metrics
    print("Testing Accuracy: {0:.4f}".format(metrics.accuracy_score(y_test, lr_predict_test)))
    print("Confusion Matrix")
    print("{0}".format(metrics.confusion_matrix(y_test, lr_predict_test)))
    #left column: predicted false    right column: predicted true
    #top row: actual false    #bottom row: actual true
    print()
    print("Classification Report")
    print(metrics.classification_report(y_test, lr_predict_test))

    #find best C hyperparameter
    C_start = 0.1
    C_end = 5
    C_inc = 0.1

    C_values, recall_scores = [], []

    C_val = C_start
    best_recall_score = 0
    while (C_val<C_end):
        C_values.append(C_val)
        lr_model_loop = LogisticRegression(C=C_val, random_state=42)
        lr_model_loop.fit(X_train, y_train.ravel())
        lr_predict_loop_test = lr_model_loop.predict(X_test)
        recall_score = metrics.recall_score(y_test, lr_predict_loop_test) #get recall score
        recall_scores.append(recall_score)
        if (recall_score > best_recall_score): #replace with highest recall
            best_recall_score = recall_score
            best_lr_predict_test = lr_predict_loop_test
        
        C_val = C_val + C_inc #raise C value
        
    best_score_C_val = C_values[recall_scores.index(best_recall_score)]
    print("1st max value of {0:.3f} occured at C={1:.3f}".format(best_recall_score, best_score_C_val))

    #Logistic Regression with class_weight = balanced
    C_start = 0.1
    C_end = 5
    C_inc = 0.1

    C_values, recall_scores = [], []

    C_val = C_start
    best_recall_score = 0
    while (C_val<C_end):
        C_values.append(C_val)
        lr_model_loop = LogisticRegression(C=C_val, class_weight='balanced', random_state=42)
        lr_model_loop.fit(X_train, y_train.ravel())
        lr_predict_loop_test = lr_model_loop.predict(X_test)
        recall_score = metrics.recall_score(y_test, lr_predict_loop_test) #get recall score
        recall_scores.append(recall_score)
        if (recall_score > best_recall_score): #replace with highest recall
            best_recall_score = recall_score
            best_lr_predict_test = lr_predict_loop_test
        
        C_val = C_val + C_inc #raise C value
        
    best_score_C_val2 = C_values[recall_scores.index(best_recall_score)]
    print("1st max value of {0:.3f} occured at C={1:.3f}".format(best_recall_score, best_score_C_val2))

    #in example, class_weight='balanced' helped, but here it did not.
    #Thus, use the previous best_score_C_val
    from sklearn.linear_model import LogisticRegression
    lr_model = LogisticRegression(C=best_score_C_val, random_state=42)
    lr_model.fit(X_train, y_train.ravel())
    lr_predict_test = lr_model.predict(X_test)

    #training metrics
    print("Accuracy: {0:.4f}".format(metrics.accuracy_score(y_test, lr_predict_test)))
    print("Confusion Matrix")
    print("{0}".format(metrics.confusion_matrix(y_test, lr_predict_test)))
    #left column: predicted false    right column: predicted true
    #top row: actual false    #bottom row: actual true
    print()
    print("Classification Report")
    print(metrics.classification_report(y_test, lr_predict_test))

    #Logistic Regression w/ CV
    from sklearn.linear_model import LogisticRegressionCV
    lr_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs=3, cv=10, refit=False, class_weight='balanced')
    #Cs means number of Cs tried to find the best regularization param
    #cv is number of folds for validation
    lr_cv_model.fit(X_train, y_train.ravel())

    lr_cv_predict_test = lr_cv_model.predict(X_test)

    #training metrics
    print("Testing Accuracy: {0:.4f}".format(metrics.accuracy_score(y_test, lr_cv_predict_test)))
    print("Confusion Matrix")
    print("{0}".format(metrics.confusion_matrix(y_test, lr_cv_predict_test)))
    #left column: predicted false    right column: predicted true
    #top row: actual false    #bottom row: actual true
    print()
    print("Classification Report")
    print(metrics.classification_report(y_test, lr_cv_predict_test))

    #Prediction
    from sklearn.datasets.samples_generator import make_blobs

    # generate 2d classification dataset
    X, y = make_blobs(n_samples=50, centers=2, n_features=17, random_state=1)
    print("X_train shape:", X_train.shape) 
    print("X shape:", X.shape) 
    # fit final model
    model = LogisticRegression()
    model.fit(X, y)
    # define one new instance
    from file import File
    import os
    path = os.path.join("C:/", "VulnerabilityResearch", "CalendarCommon.php")
    newFile = File(path, "CalendarCommon")
    fileAttributes = csvWrite.getRowData(newFile)
    fileAttributes = fileAttributes[2:] #cut out name and vulnerability
    Xnew = [fileAttributes]
    # make a prediction
    ynew = model.predict(Xnew)
    print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))
    # make a probability prediction
    ynew = model.predict_proba(Xnew)
    print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))

    #Prediction w/ trained lr_cv_model
    # define one new instance
    from file import File
    import os
    path = os.path.join("C:/", "VulnerabilityResearch", "CalendarCommon.php")
    newFile = File(path, "CalendarCommon")
    fileAttributes = csvWrite.getRowData(newFile)
    fileAttributes = fileAttributes[2:] #cut out name and vulnerability
    Xnew = [fileAttributes]
    # make a prediction
    ynew = lr_cv_model.predict(Xnew)
    print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))
    # make a probability prediction
    ynew = lr_cv_model.predict_proba(Xnew)
    print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))

    #Prediction w/ trained lr_model
    # define one new instance
    from file import File
    import os
    path = os.path.join("C:/", "VulnerabilityResearch", "CalendarCommon.php")
    newFile = File(path, "CalendarCommon")
    fileAttributes = csvWrite.getRowData(newFile)
    fileAttributes = fileAttributes[2:] #cut out name and vulnerability
    Xnew = [fileAttributes]
    # make a prediction
    ynew = lr_model.predict(Xnew)
    print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))
    # make a probability prediction
    ynew = lr_model.predict_proba(Xnew)
    print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))

    
