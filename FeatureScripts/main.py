from file import File
import os

#data containers
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#classifier models/tools
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC  
from sklearn import tree
from sklearn.model_selection import KFold # import KFold
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import scale
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectKBest #for feature selection
from sklearn.feature_selection import chi2

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D
from keras.layers import LSTM
from keras.layers import SimpleRNN

#for getting data from csv
import csvWrite
import fileRetriever
import bagOfWords

import math

#for word2vec
import gensim

#saving/loading models
import pickle

from sklearn.feature_extraction.text import TfidfVectorizer
LabeledSentence = gensim.models.doc2vec.LabeledSentence #before feeding token list to word2vec, they must be LabeledSentences

#function for feature selection
def removeFeatures(data_frame, feature_col_names): #data_frame: pd dataframe, feature_col_names: list
    '''remove unimportant features'''
    data_frame_tmp = data_frame
    feature_col_names_tmp = feature_col_names
    
    badFeatures = ['flaw', 'unsafe', 'vehicles', 'none', 'accepts', 'tiki', 'drivers', 'offset']
    for feature in badFeatures:
        if feature in data_frame_tmp.columns:
            data_frame_tmp.drop([feature], 1, inplace=True)
            feature_col_names_tmp.remove(feature)
    
    return data_frame_tmp, feature_col_names_tmp

def printBestFeatures(feature_col_names):
    '''print out top 10 features, given column names'''
    printFeatures = input('Print best features? (y/n) ')
    if printFeatures == 'y':        
        #apply SelectKBest class to extract top 10 best features
        bestfeatures = SelectKBest(score_func=chi2, k=10)
        fit = bestfeatures.fit(X,y)
        dfscores = pd.DataFrame(fit.scores_)
        dfcolumns = pd.DataFrame(feature_col_names)
        #concat two dataframes for better visualization 
        featureScores = pd.concat([dfcolumns,dfscores],axis=1)
        featureScores.columns = ['Specs','Score']  #naming the dataframe columns
        top10Features = featureScores.nlargest(10,'Score')  #10 best features
        print(top10Features)
    else:
        print("No features printed")

def reshape_y_nn(y):
    '''reshape y into two columns for neural networks'''

    #get a nonvulnerable column for y labels
    y_flipped = np.array(y, copy=True)
    for i in range(len(y_flipped)):
        if y_flipped[i][0] == 0:
            y_flipped[i][0] = 1
        elif y_flipped[i][0] == 1:
            y_flipped[i][0] = 0

    new_y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner
    return new_y

def chooseModelType(X, y):
    #options = []
    
    print("Choose model type by entering number:")
    print("0: All Models")
    print("1: Naive-Bayes")
    print("2: Decision Tree")
    print("3: Random Forest")
    print("4: Support Vector Machine")
    print("5: Logistic Regression")
    print("6: Multilayer Perceptron")
    print("7: Recurrent Neural Network")
    print("8: Long Short-Term Memory")
    print("9: Convolutional Neural Network")
    user_input = int(input())
    print("Training classifier...")
    if user_input == 0:
        cross_validation("nb", X, y)
        cross_validation("dt", X, y)
        cross_validation("rf", X, y)
        cross_validation("svm", X, y)
        cross_validation("lr", X, y)
        #reshape y for neural networks
        new_y = reshape_y_nn(y)     
        cross_validation("mlp", X, new_y)
        #reshape y for neural networks
        new_y = reshape_y_nn(y)     
        cross_validation("rnn", X, new_y)
        #reshape y for neural networks
        new_y = reshape_y_nn(y)
        cross_validation("lstm", X, new_y)
        #reshape y for neural networks
        new_y = reshape_y_nn(y)     
        cross_validation("cnn", X, new_y)
    elif user_input == 1:
        cross_validation("nb", X, y)
    elif user_input == 2:
        cross_validation("dt", X, y)
    elif user_input == 3:
        cross_validation("rf", X, y)
    elif user_input == 4:   
        cross_validation("svm", X, y)
    elif user_input == 5:
        cross_validation("lr", X, y)
    elif user_input == 6:
        #reshape y for neural networks
        new_y = reshape_y_nn(y)
        cross_validation("mlp", X, new_y)
    elif user_input == 7:
        #reshape y for neural networks
        new_y = reshape_y_nn(y)
        cross_validation("rnn", X, new_y)
    elif user_input == 8:
        #reshape y for neural networks
        new_y = reshape_y_nn(y)
        cross_validation("lstm", X, new_y)
    elif user_input == 9:
        #reshape y for neural networks
        new_y = reshape_y_nn(y)
        cross_validation("cnn", X, new_y)
    else:
        print("Input not understood.")

    print("Training completed")

def cross_validation(modelType, X, y):
    '''Cross validation for various model types. modelType is a string for the model being used'''
    # prepare the k-fold cross-validation configuration
    n_folds = 10
    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle

    # cross validation estimation of performance
    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()

    for train_ix, test_ix in kfold.split(X):
        # select samples
        trainX, trainy = X[train_ix], y[train_ix]
        testX, testy = X[test_ix], y[test_ix]
        # evaluate model
        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)

        print("trainX shape:", trainX.shape)
        print("testX shape:", testX.shape)
        print("trainy shape:", trainy.shape)
        print("testy shape:", testy.shape)
        
        if modelType == "nb":
            model, test_acc, precision, recall, f1_score = evaluate_model_nb(trainX, trainy, testX, testy)
        elif modelType == "dt":
            model, test_acc, precision, recall, f1_score = evaluate_model_dt(trainX, trainy, testX, testy)
        elif modelType == "rf":
            model, test_acc, precision, recall, f1_score = evaluate_model_rf(trainX, trainy, testX, testy)
            #print(model.feature_importances_)
        elif modelType == "svm":
            model, test_acc, precision, recall, f1_score = evaluate_model_svm(trainX, trainy, testX, testy)
        elif modelType == "lr":
            model, test_acc, precision, recall, f1_score = evaluate_model_lr(trainX, trainy, testX, testy)
        elif modelType == "mlp":
            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(trainX, trainy, testX, testy)
        elif modelType == "rnn":
            # reshape input to be [samples, time steps, features]
            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_rnn(trainX, trainy, testX, testy)
        elif modelType == "lstm":
            # reshape input to be [samples, time steps, features]
            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_lstm(trainX, trainy, testX, testy)
        elif modelType == "cnn":
            # reshape input to be [samples, time steps, features]
            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_cnn(trainX, trainy, testX, testy)
            
        scores.append(test_acc)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1_score)
        members.append(model)

    # summarize expected performance
    print(modelType, "Performance")
    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))
    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))
    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))
    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))

def chooseModelType_word2vec(X, y):
    print("Choose model type by entering number:")
    print("0: All Models")
    print("1: Multilayer Perceptron")
    print("2: Recurrent Neural Network")
    print("3: Long Short-Term Memory")
    print("4: Convolutional Neural Network")
    user_input = int(input())
    print("Training classifier...")
    if user_input == 0:    
        cross_validation_word2vec("mlp", X, y)
        cross_validation_word2vec("rnn", X, y)
        cross_validation_word2vec("lstm", X, y)
        cross_validation_word2vec("cnn", X, y)
    elif user_input == 1:   
        cross_validation_word2vec("mlp", X, y)
    elif user_input == 2:   
        cross_validation_word2vec("rnn", X, y)
    elif user_input == 3:
        cross_validation_word2vec("lstm", X, y)
    elif user_input == 4:   
        cross_validation_word2vec("cnn", X, y)
    else:
        print("Input not understood.")

    print("Training completed")

def cross_validation_word2vec(modelType, X, y):
    '''Cross validation for various model types. modelType is a string for the model being used'''
    # prepare the k-fold cross-validation configuration
    n_folds = 10
    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle

    # cross validation estimation of performance
    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()

    for train_ix, test_ix in kfold.split(X):
        # select samples
        trainX, trainy = X[train_ix], y[train_ix]
        testX, testy = X[test_ix], y[test_ix]

        #building word2vec model
        LabeledSentence = gensim.models.doc2vec.LabeledSentence #before feeding token list to word2vec, they must be LabeledSentences
            
        trainX = labelizeWords(trainX, 'TRAIN')
        testX = labelizeWords(testX, 'TEST')

        n_dim = 150

        word2vec_model = gensim.models.Word2Vec(size=n_dim, window=10, min_count=10, workers=10)
        word2vec_model.build_vocab([X.words for X in trainX])
        word2vec_model.train([X.words for X in trainX], total_examples=len(trainX), epochs=5)


        #BUILDING THE CLASSIFIER
        #building tf-idf vectorizer
        print('building tf-idf matrix ...')
        vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)
        matrix = vectorizer.fit_transform([X.words for X in trainX])
        tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
        print('vocab size :', len(tfidf))

        #scale columns in X_train and X_test
        train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim, word2vec_model, tfidf) for z in map(lambda x: x.words, trainX)])
        train_vecs_w2v = scale(train_vecs_w2v)

        test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim, word2vec_model, tfidf) for z in map(lambda x: x.words, testX)])
        test_vecs_w2v = scale(test_vecs_w2v)

        print("train_vecs_w2v shape:", train_vecs_w2v.shape)
        print("test_vecs_w2v shape:", test_vecs_w2v.shape)
        print("trainy shape:", trainy.shape)
        print("testy shape:", testy.shape)
        
        # evaluate model
        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)
        
        if modelType == "mlp":
            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        elif modelType == "rnn":
            # reshape input to be [samples, time steps, features]
            train_vecs_w2v = np.reshape(train_vecs_w2v, (train_vecs_w2v.shape[0], 1, train_vecs_w2v.shape[1]))
            test_vecs_w2v = np.reshape(test_vecs_w2v, (test_vecs_w2v.shape[0], 1, test_vecs_w2v.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_rnn(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        elif modelType == "lstm":
            # reshape input to be [samples, time steps, features]
            train_vecs_w2v = np.reshape(train_vecs_w2v, (train_vecs_w2v.shape[0], 1, train_vecs_w2v.shape[1]))
            test_vecs_w2v = np.reshape(test_vecs_w2v, (test_vecs_w2v.shape[0], 1, test_vecs_w2v.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_lstm(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        elif modelType == "cnn":
            # reshape input to be [samples, time steps, features]
            train_vecs_w2v = np.reshape(train_vecs_w2v, (train_vecs_w2v.shape[0], 1, train_vecs_w2v.shape[1]))
            test_vecs_w2v = np.reshape(test_vecs_w2v, (test_vecs_w2v.shape[0], 1, test_vecs_w2v.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_cnn(train_vecs_w2v, trainy, test_vecs_w2v, testy)
        scores.append(test_acc)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1_score)
        members.append(model)

    # summarize expected performance
    print(modelType, "Performance")
    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))
    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))
    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))
    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))

def evaluate_model_nb(X_train, y_train, X_test, y_test):
    # define model
    #create Gaussian Naive Bayes model object and train it with the data
    model = GaussianNB()
    # fit model
    model.fit(X_train, y_train.ravel())

    # save model
    filename = 'nb.sav'
    pickle.dump(model, open(filename, 'wb'))
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_dt(X_train, y_train, X_test, y_test):
    # define model
    #create Decision Tree model object and train it with the data
    model = tree.DecisionTreeClassifier()
    # fit model
    model.fit(X_train, y_train.ravel())
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_rf(X_train, y_train, X_test, y_test):
    # define model
    #create Random Classifier model object and train it with the data
    model = RandomForestClassifier(random_state=42)
    # fit model
    #model.fit(X_train, y_train.ravel())
    model.fit(X_train, y_train)

    # save model
    filename = 'rf.sav'
    pickle.dump(model, open(filename, 'wb'))
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_svm(X_train, y_train, X_test, y_test):
    # define model
    #create Support Vector Machine model object and train it with the data
    model = SVC(kernel='linear')
    # fit model
    model.fit(X_train, y_train.ravel())
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_lr(X_train, y_train, X_test, y_test):
    # define model
    #create Logistic Regression model object and train it with the data
    model = LogisticRegression(C=0.7, random_state=42) #C is regularization hyperparameter
    # fit model
    model.fit(X_train, y_train.ravel())

    # save model
    filename = 'lr.sav'
    pickle.dump(model, open(filename, 'wb'))
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_mlp(X_train, y_train, X_test, y_test):
    # define model
    #create MLP model object and train it with the data
    model = keras.Sequential([
    #start with layer of input shape (anything, 17)
    #keras.layers.Dense(500, input_shape=(csvWrite.getNumFeatures(),), activation=tf.nn.relu),
    keras.layers.Dense(500, input_shape=(len(X_train[0]),), activation=tf.nn.relu), #len(X_train[0]) is the num of 
    keras.layers.Dense(16, activation=tf.nn.relu),
    keras.layers.Dense(2, activation=tf.nn.sigmoid) #keras.layers.Dense(1, activation=tf.nn.sigmoid)
])
    model.compile(optimizer='adam', 
              loss='binary_crossentropy',
              metrics=['accuracy'])
    
    # fit model
    history = model.fit(X_train, y_train, epochs=5)

    # save model
    '''filename = 'mlp.sav'
    pickle.dump(model, open(filename, 'wb'))'''
    #model.save('mlp.h5')
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    predicted = np.argmax(predict_test, axis=1)
    
    report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)
    #Accuracy
    test_loss, test_acc = model.evaluate(X_test, y_test)
    precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_rnn(trainX, trainy, testX, testy):
    #dataDimensions = csvWrite.getNumFeatures()
    dataDimensions = len(trainX[0][0])
    
    # create and fit the RNN network
    model = Sequential()
    model.add(SimpleRNN(4, input_shape=(1, dataDimensions)))
    model.add(Dense(2)) #output layer, vulner or nonvulner
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(trainX, trainy, epochs=5, batch_size=1, verbose=2)
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(testX)
    
    predicted = np.argmax(predict_test, axis=1)

    #metrics
    report = metrics.classification_report(np.argmax(testy, axis=1), predicted)
    #test_loss, test_acc = model.evaluate(testX, testy)
    test_acc = metrics.accuracy_score(np.argmax(testy, axis=1), predicted)
    precision = metrics.precision_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_lstm(trainX, trainy, testX, testy):
    #dataDimensions = csvWrite.getNumFeatures()
    dataDimensions = len(trainX[0][0])
    
    # create and fit the LSTM network
    model = Sequential()
    model.add(LSTM(4, input_shape=(1, dataDimensions)))
    model.add(Dense(2)) #output layer, vulner or nonvulner
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(trainX, trainy, epochs=5, batch_size=1, verbose=2)

    # save model
    '''filename = 'lstm.sav'
    pickle.dump(model, open(filename, 'wb'))'''
    #model.save('lstm.h5')
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(testX)
    
    predicted = np.argmax(predict_test, axis=1)

    #metrics
    report = metrics.classification_report(np.argmax(testy, axis=1), predicted)
    test_acc = metrics.accuracy_score(np.argmax(testy, axis=1), predicted)
    precision = metrics.precision_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_cnn(trainX, trainy, testX, testy):
    #dataDimensions = csvWrite.getNumFeatures()
    feature_number = len(trainX[0][0]) #count of features
    
    # create and fit the LSTM network
    model = Sequential()
    model.add(Conv1D(64, 1, activation='relu', input_shape=(None,feature_number)))
    model.add(Conv1D(64, 1, activation='relu'))
    model.add(MaxPooling1D(1))
    model.add(Conv1D(128, 1, activation='relu'))
    model.add(Conv1D(128, 1, activation='relu'))
    model.add(GlobalAveragePooling1D())
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='sigmoid'))
    
    model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
              
    model.fit(trainX, trainy, batch_size=16, epochs=5)

    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(testX)
    
    predicted = np.argmax(predict_test, axis=1)

    #metrics
    report = metrics.classification_report(np.argmax(testy, axis=1), predicted)
    test_acc = metrics.accuracy_score(np.argmax(testy, axis=1), predicted)
    precision = metrics.precision_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def getCorpus():
    files = fileRetriever.getFileSet(2)
    corpus = []
    for i, file in enumerate(files):
        wordList = []
        with open(file.dir, "r", encoding="utf8") as f:
            text = f.read()
            text = bagOfWords.filterComments(text)
            wordList = gensim.utils.simple_preprocess(text) #maybe add min_len and max_len params
            corpus.append(wordList)

    return corpus
    
def labelizeWords(words, label_type):
    '''label corpus words'''
    labelized = []
    for i,v in enumerate(words):
        label = '%s_%s'%(label_type,i)
        labelized.append(LabeledSentence(v, [label]))
    return labelized

def buildWordVector(tokens, size, model, tfidf):
    '''given list of tokens, creates averaged vector. Used to turn X_train and X_test into vector list'''
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += model[word].reshape((1, size)) * tfidf[word]
            count += 1.
        except KeyError: # handling the case where the token is not
                         # in the corpus. useful for testing.
            continue
    if count != 0:
        vec /= count

    #print("Word Vector Shape:")
    #print(vec.shape)
    return vec

def printMenu():
    print("SQL Injection Classifier Menu")
    print("1. Retrain Classifier")
    print("2. Test File Set (Not implemented yet)")
    print("3. Quit")



#PROGRAM START
print("Welcome to the SQL injection machine learning classifier!")
loop = True

while loop:
    printMenu()
    choice = int(input("Enter your choice: "))

    if choice == 1:
        print("Will retrain classifier")

        print("Rewrite fileInfo.csv? Contains feature information for file set ")
        csvWrite.rewriteCSV()

        #get data frame of features
        csvDirectory = csvWrite.getCSVDirectory()
        data_frame = pd.read_csv(csvDirectory)    
        
        featureSetChoice = int(input("Train bagofwords (0) or word2vec (1) implementation? "))
        if featureSetChoice == 0:

            #divide training and testing set
            feature_col_names, predicted_class_names = csvWrite.getFeatureColumns()

            #data frame filtering
            data_frame, feature_col_names = removeFeatures(data_frame, feature_col_names)

            #establish X and y data sets
            X = data_frame[feature_col_names].values # predictor feature columns (~500 x m)
            y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

            #print most important features
            printBestFeatures(feature_col_names)

            print("Starting classifier training")
            chooseModelType(X, y)
        elif featureSetChoice == 1:
            print("Please make sure you have a C compiler, like MinGW. If not, program will now fail.")
            assert gensim.models.doc2vec.FAST_VERSION > -1 #ensure fast performance. Need to download C compiler, like MinGW

            #get corpus
            corpus = getCorpus()

            #establish X and y data sets
            X = np.array(corpus)
            _not_used, predicted_class_names = csvWrite.getFeatureColumns()
            y = data_frame[predicted_class_names].values

            #get a nonvulnerable column for y labels
            y_flipped = np.array(y, copy=True)
            for i in range(len(y_flipped)):
                if y_flipped[i][0] == 0:
                    y_flipped[i][0] = 1
                elif y_flipped[i][0] == 1:
                    y_flipped[i][0] = 0
                    
            y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner

            print("Starting classifier training")
            chooseModelType_word2vec(X, y)

    elif choice == 2:
        def getUserFileDirectory():
            #FIXME: For single file, check if file has correct extension
            print("Enter a full directory or file path")
            print("ex: C:\VulnerabilityResearch\FileSet\Vulnerable\addressbook_family.inc") 
            userDir = input()
            assert os.path.exists(userDir), "I did not find the file at, "+str(userDir)
            return userDir

        userDir = getUserFileDirectory()
        if os.path.isfile(userDir):
            #may need to change userdir to use full file directory
            _unused, name = os.path.split(userDir)
            newFile = File(userDir, name)

            #write features to file
            csvWrite.extractFeatures([newFile])

            #get data frame of features
            csvDirectory = csvWrite.getCSVDirectory(getUserFile=True)
            data_frame = pd.read_csv(csvDirectory)


            
            

            feature_col_names, predicted_class_names = csvWrite.getFeatureColumns()
            #data frame filtering
            data_frame, feature_col_names = removeFeatures(data_frame, feature_col_names)

            #establish X and y data sets
            X = data_frame[feature_col_names].values # predictor feature columns (~500 x m)
            y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

            #predict against model
            model = pickle.load(open("nb.sav", 'rb'))
            predict_test = model.predict(X)
            test_acc = metrics.accuracy_score(y, predict_test)
            
        elif os.path.isdir(userdir):
            pass

    elif choice == 3:
        print("Quitting program")
        loop = False
    else:
        print("Input not understood. Please try again")
