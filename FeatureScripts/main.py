#data containers
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#classifier models/tools
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold # import KFold
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectKBest #for feature selection
from sklearn.feature_selection import chi2

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

#for getting data from csv
import csvWrite
import math


#function for feature selection
def removeFeatures(data_frame, feature_col_names): #data_frame: pd dataframe, feature_col_names: list
    '''remove unimportant features'''
    data_frame_tmp = data_frame
    feature_col_names_tmp = feature_col_names
    
    badFeatures = ['flaw', 'unsafe', 'vehicles', 'none', 'accepts', 'tiki', 'drivers', 'offset']
    for feature in badFeatures:
        if feature in data_frame_tmp.columns:
            data_frame_tmp.drop([feature], 1, inplace=True)
            feature_col_names_tmp.remove(feature)
    
    return data_frame_tmp, feature_col_names_tmp

def printBestFeatures(feature_col_names):
    '''print out top 10 features, given column names'''
    printFeatures = input('Print best features? y/n ')
    if printFeatures == 'y':        
        #apply SelectKBest class to extract top 10 best features
        bestfeatures = SelectKBest(score_func=chi2, k=10)
        fit = bestfeatures.fit(X,y)
        dfscores = pd.DataFrame(fit.scores_)
        dfcolumns = pd.DataFrame(feature_col_names)
        #concat two dataframes for better visualization 
        featureScores = pd.concat([dfcolumns,dfscores],axis=1)
        featureScores.columns = ['Specs','Score']  #naming the dataframe columns
        top10Features = featureScores.nlargest(10,'Score')  #10 best features
        print(top10Features)
    else:
        print("No features printed")

def chooseModelType(X, y):
    print("Choose model type by entering number:")
    print("1: Naive-Bayes")
    print("2: Random Forest")
    print("3: Logistic Regression")
    print("4: Multilayer Perceptron")
    print("5: Long Short-Term Memory")
    user_input = int(input())
    if user_input == 1:
        cross_validation("nb", X, y)
    elif user_input == 2:
        cross_validation("rf", X, y)
    elif user_input == 3:
        cross_validation("lr", X, y)
    elif user_input == 4:
        cross_validation("mlp", X, y)
    elif user_input == 5:
        cross_validation("lstm", X, y)
    else:
        pass

def cross_validation(modelType, X, y):
    '''Cross validation for various model types. modelType is a string for the model being used'''
    #FIXME: Refactor to pass in X and y
    
    # prepare the k-fold cross-validation configuration
    n_folds = 10
    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle

    # cross validation estimation of performance
    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()

    for train_ix, test_ix in kfold.split(X):
        # select samples
        trainX, trainy = X[train_ix], y[train_ix]
        testX, testy = X[test_ix], y[test_ix]
        # evaluate model
        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)
        
        if modelType == "nb":
            model, test_acc, precision, recall, f1_score = evaluate_model_nb(trainX, trainy, testX, testy)
        elif modelType == "rf":
            model, test_acc, precision, recall, f1_score = evaluate_model_rf(trainX, trainy, testX, testy)
            print(model.feature_importances_)
        elif modelType == "lr":
            model, test_acc, precision, recall, f1_score = evaluate_model_lr(trainX, trainy, testX, testy)
        elif modelType == "mlp":
            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(trainX, trainy, testX, testy)
        elif modelType == "lstm":
            # reshape input to be [samples, time steps, features]
            trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
            testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
            
            model, test_acc, precision, recall, f1_score = evaluate_model_lstm(trainX, trainy, testX, testy)
        scores.append(test_acc)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1_score)
        members.append(model)

    # summarize expected performance
    print(modelType, "Performance")
    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))
    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))
    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))
    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))

def evaluate_model_nb(X_train, y_train, X_test, y_test):
    # define model
    #create Gaussian Naive Bayes model object and train it with the data
    model = GaussianNB()
    # fit model
    model.fit(X_train, y_train.ravel())
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_rf(X_train, y_train, X_test, y_test):
    # define model
    #create Random Classifier model object and train it with the data
    model = RandomForestClassifier(random_state=42)
    # fit model
    model.fit(X_train, y_train.ravel())
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_lr(X_train, y_train, X_test, y_test):
    # define model
    #create Logistic Regression model object and train it with the data
    model = LogisticRegression(C=0.7, random_state=42) #C is regularization hyperparameter
    # fit model
    model.fit(X_train, y_train.ravel())
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    #Accuracy
    test_acc = metrics.accuracy_score(y_test, predict_test)
    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)
    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)
    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_mlp(X_train, y_train, X_test, y_test):
    # define model
    #create MLP model object and train it with the data
    model = keras.Sequential([
    #start with layer of input shape (anything, 17)
    #keras.layers.Dense(500, input_shape=(csvWrite.getNumFeatures(),), activation=tf.nn.relu),
    keras.layers.Dense(500, input_shape=(len(X_train[0]),), activation=tf.nn.relu), #len(X_train[0]) is the num of 
    keras.layers.Dense(16, activation=tf.nn.relu),
    keras.layers.Dense(2, activation=tf.nn.sigmoid) #keras.layers.Dense(1, activation=tf.nn.sigmoid)
])
    model.compile(optimizer='adam', 
              loss='binary_crossentropy',
              metrics=['accuracy'])
    
    # fit model
    history = model.fit(X_train, y_train, epochs=5)
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(X_test)

    predicted = np.argmax(predict_test, axis=1)
    
    report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)
    #Accuracy
    test_loss, test_acc = model.evaluate(X_test, y_test)
    precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

def evaluate_model_lstm(trainX, trainy, testX, testy):
    #dataDimensions = csvWrite.getNumFeatures()
    dataDimensions = len(trainX[0][0])
    
    # create and fit the LSTM network
    model = Sequential()
    model.add(LSTM(4, input_shape=(1, dataDimensions)))
    model.add(Dense(2)) #output layer, vulner or nonvulner
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(trainX, trainy, epochs=5, batch_size=1, verbose=2)
    
    # evaluate the model
    #predict values using the testing data
    predict_test = model.predict(testX)
    
    predicted = np.argmax(predict_test, axis=1)

    #metrics
    report = metrics.classification_report(np.argmax(testy, axis=1), predicted)
    test_acc = metrics.accuracy_score(np.argmax(testy, axis=1), predicted)
    precision = metrics.precision_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    recall = metrics.recall_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    f1_score = metrics.f1_score(np.argmax(testy, axis=1), predicted, pos_label = 1)
    
    return model, test_acc, precision, recall, f1_score

#PROGRAM START
print("Welcome to the SQL injection machine learning classifier")

#choose between implementing own files and using files already in FileSet folder
fileSetChoice = int(input("Use preset files (0) or input own files (1)? "))
if fileSetChoice == 0:
    featureSetChoice = int(input("Use bagofwords (0) or word2vec (1)? "))
    if featureSetChoice == 0:
        print("Rewrite fileInfo.csv? Contains feature information for file set ")
        csvWrite.rewriteCSV()

        #get data frame of features
        csvDirectory = csvWrite.getCSVDirectory()
        data_frame = pd.read_csv(csvDirectory)

        #divide training and testing set
        feature_col_names, predicted_class_names = csvWrite.getFeatureColumns()

        #data frame filtering
        data_frame, feature_col_names = removeFeatures(data_frame, feature_col_names)

        #establish X and y data sets
        X = data_frame[feature_col_names].values # predictor feature columns (~500 x m)
        y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)

        #print most important features
        printBestFeatures(feature_col_names)

        print("Starting classifier training")
        chooseModelType(X, y)
        
    elif featureSetChoice == 1:
        print("Please make sure you have a C compiler, like MinGW. If not, program will now fail.")
        assert gensim.models.doc2vec.FAST_VERSION > -1 #ensure fast performance. Need to download C compiler, like MinGW

        #get corpus
    else:
        print("Choice not understood.")
elif fileSetChoice == 1:
    pass
else:
    print("Choice not understood. Please restart.")
