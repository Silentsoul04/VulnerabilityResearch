{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import csvWrite\n",
    "\n",
    "#do plotting inline\n",
    "%matplotlib inline\n",
    "\n",
    "#get directory\n",
    "csvDirectory = csvWrite.getCSVDirectory()\n",
    "\n",
    "data_frame = pd.read_csv(csvDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col_names = csvWrite.getRowHeaders()\n",
    "feature_col_names = feature_col_names[2:] #all except name and vulnerable column\n",
    "predicted_class_names = ['Vulnerable']\n",
    "\n",
    "X = data_frame[feature_col_names].values # predictor feature columns (17 x m)\n",
    "y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(modelType):\n",
    "    #modelType is a string for the model being used\n",
    "    \n",
    "    # prepare the k-fold cross-validation configuration\n",
    "    n_folds = 10\n",
    "    kfold = KFold(n_folds, True, 1) #n_splits, random_state, shuffle\n",
    "\n",
    "    # cross validation estimation of performance\n",
    "    scores, precisions, recalls, f1_scores, members = list(), list(), list(), list(), list()\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        # select samples\n",
    "        trainX, trainy = X[train_ix], y[train_ix]\n",
    "        testX, testy = X[test_ix], y[test_ix]\n",
    "        # evaluate model\n",
    "        model, test_acc, precision, recall, f1_score = (0, 0, 0, 0, 0)\n",
    "        \n",
    "        if modelType == \"nb\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_nb(trainX, trainy, testX, testy)\n",
    "        elif modelType == \"rf\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_rf(trainX, trainy, testX, testy)\n",
    "        elif modelType == \"lr\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_lr(trainX, trainy, testX, testy)\n",
    "        elif modelType == \"mlp\":\n",
    "            model, test_acc, precision, recall, f1_score = evaluate_model_mlp(trainX, trainy, testX, testy)\n",
    "        #print('Accuracy >%.3f' % test_acc)\n",
    "        #print('Precision >%.3f' % precision)\n",
    "        #print('Recall >%.3f' % recall)\n",
    "        #print('F1-Scores >%.3f' % f1_score)\n",
    "        scores.append(test_acc)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "        members.append(model)\n",
    "\n",
    "    # summarize expected performance\n",
    "    print(modelType, \"Performance\")\n",
    "    print('Estimated Accuracy: %.3f, Std Ddv: (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "    print('Estimated Precision: %.3f, Std Ddv: (%.3f)' % (np.mean(precisions), np.std(precisions)))\n",
    "    print('Estimated Recall: %.3f, Std Ddv: (%.3f)' % (np.mean(recalls), np.std(recalls)))\n",
    "    print('Estimated F1-Score: %.3f, Std Ddv: (%.3f)' % (np.mean(f1_scores), np.std(f1_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_nb(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create Gaussian Naive Bayes model object and train it with the data\n",
    "    model = GaussianNB()\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    #Accuracy\n",
    "    test_acc = metrics.accuracy_score(y_test, predict_test)\n",
    "    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)\n",
    "    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 10 Fold Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Accuracy: 0.902, Std Ddv: (0.010)\n",
      "Estimated Precision: 0.857, Std Ddv: (0.143)\n",
      "Estimated Recall: 0.136, Std Ddv: (0.036)\n",
      "Estimated F1-Score: 0.230, Std Ddv: (0.046)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(\"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_rf(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create Random Classifier model object and train it with the data\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    #Accuracy\n",
    "    test_acc = metrics.accuracy_score(y_test, predict_test)\n",
    "    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)\n",
    "    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 10 Fold Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Accuracy: 0.902, Std Ddv: (0.010)\n",
      "Estimated Precision: 0.694, Std Ddv: (0.165)\n",
      "Estimated Recall: 0.242, Std Ddv: (0.091)\n",
      "Estimated F1-Score: 0.338, Std Ddv: (0.083)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(\"rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_lr(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create Logistic Regression model object and train it with the data\n",
    "    model = LogisticRegression(C=0.7, random_state=42) #C is regularization hyperparameter\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    #Accuracy\n",
    "    test_acc = metrics.accuracy_score(y_test, predict_test)\n",
    "    precision = metrics.precision_score(y_test, predict_test, pos_label = 1)\n",
    "    recall = metrics.recall_score(y_test, predict_test, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(y_test, predict_test, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Accuracy: 0.904, Std Ddv: (0.011)\n",
      "Estimated Precision: 0.927, Std Ddv: (0.047)\n",
      "Estimated Recall: 0.125, Std Ddv: (0.028)\n",
      "Estimated F1-Score: 0.219, Std Ddv: (0.044)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(\"lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Training Data for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_frame[feature_col_names].values # predictor feature columns (17 x m)\n",
    "y = data_frame[predicted_class_names].values # predicted class (1=true, 0=false)(1 x m)\n",
    "#get a nonvulnerable column for y labels\n",
    "y_flipped = np.array(y, copy=True)\n",
    "for i in range(len(y_flipped)):\n",
    "    if y_flipped[i][0] == 0:\n",
    "        y_flipped[i][0] = 1\n",
    "    elif y_flipped[i][0] == 1:\n",
    "        y_flipped[i][0] = 0\n",
    "        \n",
    "y = np.concatenate((y_flipped, y), axis=1) #0: nonvulner, 1: vulner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_mlp(X_train, y_train, X_test, y_test):\n",
    "    # define model\n",
    "    #create MLP model object and train it with the data\n",
    "    model = keras.Sequential([\n",
    "    #start with layer of input shape (anything, 17)\n",
    "    keras.layers.Dense(500, input_shape=(csvWrite.getNumFeatures(),), activation=tf.nn.relu),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.sigmoid) #keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "    model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, y_train, epochs=5)\n",
    "    \n",
    "    # evaluate the model\n",
    "    #predict values using the testing data\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    predicted = np.argmax(predict_test, axis=1)\n",
    "    \n",
    "    #print(np.argmax(y_test, axis=1).shape)\n",
    "    #print(np.argmax(y_test, axis=1))\n",
    "    #print(predict_test.shape)\n",
    "    #print(predict_test)\n",
    "    \n",
    "    #import the performance metrics library\n",
    "    report = metrics.classification_report(np.argmax(y_test, axis=1), predicted)\n",
    "    #Accuracy\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    precision = metrics.precision_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "    recall = metrics.recall_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "    f1_score = metrics.f1_score(np.argmax(y_test, axis=1), predicted, pos_label = 1)\n",
    "    \n",
    "    return model, test_acc, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 1s 169us/sample - loss: 0.3028 - acc: 0.8984\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 109us/sample - loss: 0.2805 - acc: 0.9031\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 136us/sample - loss: 0.2791 - acc: 0.9034\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 113us/sample - loss: 0.2770 - acc: 0.9043\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 107us/sample - loss: 0.2757 - acc: 0.9054\n",
      "971/971 [==============================] - 0s 297us/sample - loss: 0.2854 - acc: 0.9001\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 1s 170us/sample - loss: 0.3282 - acc: 0.8898\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 109us/sample - loss: 0.2859 - acc: 0.9031\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 109us/sample - loss: 0.2821 - acc: 0.9037\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 115us/sample - loss: 0.2799 - acc: 0.9036\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 105us/sample - loss: 0.2776 - acc: 0.9050\n",
      "971/971 [==============================] - 0s 255us/sample - loss: 0.3043 - acc: 0.9053\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 1s 167us/sample - loss: 0.3122 - acc: 0.8970\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 109us/sample - loss: 0.2818 - acc: 0.9034\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 117us/sample - loss: 0.2793 - acc: 0.9045\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 102us/sample - loss: 0.2780 - acc: 0.9050\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 100us/sample - loss: 0.2784 - acc: 0.9048\n",
      "971/971 [==============================] - 0s 268us/sample - loss: 0.2724 - acc: 0.9053\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 1s 170us/sample - loss: 0.3086 - acc: 0.8973\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 108us/sample - loss: 0.2859 - acc: 0.9020\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 123us/sample - loss: 0.2809 - acc: 0.9035\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 118us/sample - loss: 0.2796 - acc: 0.9004\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 104us/sample - loss: 0.2786 - acc: 0.9046\n",
      "971/971 [==============================] - 0s 352us/sample - loss: 0.2626 - acc: 0.9114\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 2s 176us/sample - loss: 0.3195 - acc: 0.8962\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 108us/sample - loss: 0.2857 - acc: 0.9018\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 115us/sample - loss: 0.2814 - acc: 0.9023\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 105us/sample - loss: 0.2811 - acc: 0.9031\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 110us/sample - loss: 0.2782 - acc: 0.9044\n",
      "971/971 [==============================] - 0s 313us/sample - loss: 0.2661 - acc: 0.9150\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 2s 183us/sample - loss: 0.3052 - acc: 0.9007\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 116us/sample - loss: 0.2771 - acc: 0.9062\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 104us/sample - loss: 0.2733 - acc: 0.9069\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 121us/sample - loss: 0.2706 - acc: 0.9067\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 126us/sample - loss: 0.2709 - acc: 0.9052\n",
      "971/971 [==============================] - 1s 517us/sample - loss: 0.3355 - acc: 0.8826\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 2s 179us/sample - loss: 0.3439 - acc: 0.8862\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 108us/sample - loss: 0.2888 - acc: 0.9008\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 125us/sample - loss: 0.2835 - acc: 0.9021\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 118us/sample - loss: 0.2824 - acc: 0.9023\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 134us/sample - loss: 0.2797 - acc: 0.9056\n",
      "971/971 [==============================] - 0s 361us/sample - loss: 0.2485 - acc: 0.9156\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 2s 224us/sample - loss: 0.3200 - acc: 0.8973\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 119us/sample - loss: 0.2836 - acc: 0.9043\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 148us/sample - loss: 0.2780 - acc: 0.9050\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 128us/sample - loss: 0.2768 - acc: 0.9058\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 157us/sample - loss: 0.2755 - acc: 0.9041\n",
      "971/971 [==============================] - 0s 383us/sample - loss: 0.2953 - acc: 0.8888\n",
      "Epoch 1/5\n",
      "8738/8738 [==============================] - 2s 212us/sample - loss: 0.3314 - acc: 0.8904\n",
      "Epoch 2/5\n",
      "8738/8738 [==============================] - 1s 140us/sample - loss: 0.2846 - acc: 0.9015\n",
      "Epoch 3/5\n",
      "8738/8738 [==============================] - 1s 133us/sample - loss: 0.2807 - acc: 0.9039\n",
      "Epoch 4/5\n",
      "8738/8738 [==============================] - 1s 143us/sample - loss: 0.2792 - acc: 0.9023\n",
      "Epoch 5/5\n",
      "8738/8738 [==============================] - 1s 122us/sample - loss: 0.2777 - acc: 0.9028\n",
      "971/971 [==============================] - 0s 432us/sample - loss: 0.2755 - acc: 0.9042\n",
      "Epoch 1/5\n",
      "8739/8739 [==============================] - 2s 240us/sample - loss: 0.3321 - acc: 0.8934\n",
      "Epoch 2/5\n",
      "8739/8739 [==============================] - 1s 144us/sample - loss: 0.2837 - acc: 0.9039\n",
      "Epoch 3/5\n",
      "8739/8739 [==============================] - 1s 141us/sample - loss: 0.2789 - acc: 0.9046\n",
      "Epoch 4/5\n",
      "8739/8739 [==============================] - 1s 162us/sample - loss: 0.2770 - acc: 0.9071\n",
      "Epoch 5/5\n",
      "8739/8739 [==============================] - 1s 146us/sample - loss: 0.2754 - acc: 0.9057\n",
      "970/970 [==============================] - 0s 392us/sample - loss: 0.2894 - acc: 0.9010\n",
      "Estimated Accuracy: 0.903, Std Ddv: (0.010)\n",
      "Estimated Precision: 0.688, Std Ddv: (0.140)\n",
      "Estimated Recall: 0.273, Std Ddv: (0.113)\n",
      "Estimated F1-Score: 0.363, Std Ddv: (0.106)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
